{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI2000- Foundations of Machine Learning\n",
    "## Assignment 1\n",
    "### Tanmay Goyal\n",
    "### AI20BTECH11021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. $k-NN$ Classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)  What happens to the training error when the neighbour size $k$ varies from $n$ to 1?\n",
    "Answer: As the neighbour size $k$ varies from $n$ to 1, the training error tends to decrease. This is because for $k=1$, each training sample would get classified as itself, since the single point closest to a point is itself. Thus, the training error for a $1-NN$ model would be zero. As the value of $k$ would increase, we start taking into consideration points in the vicinity of the sample, and thus the error tends to creep in, since it might so happen that a class-1 point in the vicinity of class-2 points gets classified as a class-2 point. Outliers would contribute to the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) What happens to the generalisation error as k varies?\n",
    "Answer: For $k=1$ , the model tends to overfit the decision boundaries, since it takes into consideration the first nearest neighbour only. Thus, the generalisation error tends to be high for $k=1$. As the value of $k$ increases, more points are taken into consideration, and the decision boundaries tend to smoothen out, as we now tend to average over more data. This leads to a decrease in the generalisation error. The generalisation error tends to decrease till a certain value of $k$, after which it would again start increasing, as it starts including points from other classes.\n",
    "\n",
    "![alt text](Generalisation_error_vs_K.png \"Generalisation error vs K for KNN Models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Give two reasons why $k-NN$ is undesirable when input dimension is high.\n",
    "Answer:\n",
    "1) Because the $k-NN$ model depends on data points of same classes being close enough to one another, it also hinges on the fact that these points are close in every dimension. As the number of dimensions increase, it is difficult to guarantee this fact. It may so happen that the distance between two points in one or two dimensions may outweigh the distances in the rest of the dimensions, thus making $k-NN$ lose its predictive power.\n",
    "2) As the size of our dataset grows, more dimensions would lead to intensive calculations. Since the distance between every pair of points has to be calculated, and the distance between two points involves the calculation of the square of the differences of the coordinates in each dimension, the number of computations would increase at a rapid rate for higher dimensional data, and massive datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Is it possible to build a univariate decision tree (with decisions of the form $x>a$ or $y>b$) which classifies similar to a $1- NN$ model?\n",
    "Answer: \n",
    "It depends on the data. When the decision tree makes univariate decisions like $x>a$ or $y>b$, then the decision boundaries are parallel to the axes. The decision boundaries between points in a $1-NN$ diagram can be parallel to the axes or can be sloped lines. If the decision boundaries between points are parallel to the axes, for example, in the case of datapoints {(0,0) , (2,0) , (2,2) , (0,2)}, we can make the decisions using a decision tree. However, in the other case, we would not be able to assign any real constant in the univariate decision making process. For example, in {(0,0) , (2,0) , (1,1)}, our first decision can be if $x>1$. If yes, then to make a decision, we would have to consider $y$. However, since the line is sloped, $y$ becomes a function of $x$, and there is no one fixed constant we can take to make a decision. For every single test point, this parametre $y$ to classify it into one of the classes now becomes variable, and hence, we cannot incorporate it into a decision tree.\n",
    "Given below are the graphs representing the two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graphs for the examples mentioned above\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot([0,2],[0,0], 'ro' )\n",
    "plt.plot([2,0] , [2, 2] , 'rx')\n",
    "plt.plot([1 for i in range(-10 ,10)] , [i for i in range(-10,10)] , 'k--')\n",
    "plt.plot([i for i in range(-10,10)] , [1 for i in range(-10,10)] , 'k--')\n",
    "plt.title(\"Parallel Decision boundaries- Example\")\n",
    "plt.xlim(-1,4)\n",
    "plt.ylim(-1,4)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot([0,2] , [0,0] , 'ro') \n",
    "plt.plot(1,1, 'rx')\n",
    "plt.plot([1 for i in range(-1 , 1)] , [i for i in range(-1,1)] , 'k--')\n",
    "plt.plot([i for i in range(2)] , [-i+1 for i in range(2)] , 'k--')\n",
    "plt.plot([i for i in range(1,3)] , [i-1 for i in range(1,3)] , 'k--')\n",
    "plt.xlim(-0.2,2.2)\n",
    "plt.ylim(-0.5,2.2)\n",
    "plt.grid(True)\n",
    "plt.title(\"Sloped Decision Boundaries- Example\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) A training set consists of one dimensional examples from two classes. Class 1 consists of ${\\{0.5, 0.1, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.35, 0.25\\}}$ with variance $0.0149$ and class 2 consists of ${\\{0.9, 0.8, 0.75, 1.0\\}}$ with variance $0.0092$. Estimate the class probabilities $p_1$ and $p_2$. Also find the probability that the test point $x = 0.6$ belongs to class 1.\n",
    "\n",
    "Answer: \n",
    "Since there are $14$ training samples out of which $10$ belong to class 1 and $4$ belong to class 2:\n",
    "\n",
    "$p_1 = \\frac{10}{14}$ and $p_2 = \\frac{4}{14}$\n",
    "\n",
    "Now, mean of all samples in class 1 = $\\frac{0.5 + 0.1 + 0.2 + 0.4 + 0.3 + 0.2 + 0.2 + 0.1 + 0.35 + 0.25}{10} = 0.26$\n",
    "\n",
    "Mean of all samples in class 2 = $\\frac{0.9 + 0.8 + 0.75 + 1}{4} = 0.8625$\n",
    "\n",
    "We assume a Gaussian distribution: $p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x_i - \\mu)^2}{2\\sigma^2}}$ where $f$ is the PDF, $\\mu$ = mean and $\\sigma^2$ = variance of the data.\n",
    "\n",
    "Therefore, we get:\n",
    "$p(x = x_i | class1) = \\frac{1}{\\sqrt{2\\pi (0.0149)}} e^{-\\frac{(x_i - 0.26)^2}{2(0.0149)}}  = \\frac{1}{\\sqrt{0.0298\\pi}} e^{-\\frac{(x_i - 0.26)^2}{0.0298}} $\n",
    "\n",
    "Similarly, $p(x = x_i | class2) = \\frac{1}{\\sqrt{2\\pi (0.0092)}} e^{-\\frac{(x_i - 0.8625)^2}{2(0.0092)}}  = \\frac{1}{\\sqrt{0.0184\\pi}} e^{-\\frac{(x_i - 0.8625)^2}{0.0184}} $\n",
    "\n",
    "To find: $p(class1 | x = 0.6)$\n",
    "\n",
    "Using Bayes' Theorem, we can write \n",
    "\n",
    "$p(class1| x = 0.6) = \\frac{p(x = 0.6 | class1)p_1}{p(x = 0.6 | class1)p_1 + p(x = 0.6 | class2)p_2}$\n",
    "\n",
    "where\n",
    "\n",
    "$p(x = 0.6 | class1) = \\frac{1}{\\sqrt{0.0298\\pi}} e^{-\\frac{(0.6 - 0.26)^2}{0.0298}} = 0.0675$\n",
    "\n",
    "and \n",
    "\n",
    "$p(x = 0.6 | class2) = \\frac{1}{\\sqrt{0.0184\\pi}} e^{-\\frac{(0.6 - 0.8625)^2}{0.0184}} = 0.0983$\n",
    "\n",
    "Subsitututing the values, we get:\n",
    "\n",
    "$p(class1| x = 0.6) = \\frac{0.0675 \\times \\frac{10}{14}}{0.0675 \\times \\frac{10}{14} + 0.0983 \\times \\frac{4}{14}} = 0.6319$\n",
    "\n",
    "Thus, the required probability is $0.6319$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Text Classifier- Every document is classified as $sport$ or $politics$. The following is the row vector of attributes:\n",
    "$ x = {(goal,football,golf,defence,offence,wicket,office,strategy)}$\n",
    "#### The training data is:\n",
    "$x_{politics} = \\begin{matrix} \n",
    "1& 0& 1& 1& 1& 0& 1& 1\\\\\n",
    "0& 0& 0& 1& 0& 0& 1& 1\\\\\n",
    "1& 0& 0& 1& 1& 0& 1& 0\\\\\n",
    "0& 1& 0& 0& 1& 1& 0& 1\\\\\n",
    "0& 0& 0& 1& 1& 0& 1& 1\\\\\n",
    "0& 0& 0& 1& 1& 0& 0& 1\\\\\n",
    "\\end{matrix}$\n",
    "\n",
    "$x_{sport} = \\begin{matrix}\n",
    "1& 1& 0& 0& 0& 0& 0& 0\\\\\n",
    "0& 0& 1& 0& 0& 0& 0& 0\\\\\n",
    "1& 1& 0& 1& 0& 0& 0& 0\\\\\n",
    "1& 1& 0& 1& 0& 0& 0& 1\\\\\n",
    "1& 1& 0& 1& 1& 0& 0& 0\\\\\n",
    "0& 0& 0& 1& 0& 1& 0& 0\\\\\n",
    "\\end{matrix}$\n",
    "    \n",
    "#### Find the probability that document $x = {(1,0,0,1,1,1,1,0)}$ is about $politics$.    \n",
    "    \n",
    "    \n",
    "Answer: Let the attributes be represented by $A$. Then we have to find $p(politics | A)$. \n",
    "\n",
    "Using Bayes' Theorem:\n",
    "$p(politics | A) = \\frac{p(A|politics)p(politics)}{p(A)} = \\frac{p(A|politics)p(politics)}{p(A|politics)p(politics) + p(A|sport)p(sport)}$\n",
    "\n",
    "We see that $A$ requires us to have a $1$ in the 7th column. However, in $x_{sport}$, we see that none of the documents consist of a $1$ in the 7th column. Therefore, while finding $p(A|sport)$, we would get $0$. Thus, our required probability reduces to :\n",
    "\n",
    "$p(politics | A) = \\frac{p(A|politics)p(politics)}{p(A|politics)p(politics) + p(A|sport)p(sport)} = \\frac{p(A|politics)p(politics)}{p(A|politics)p(politics) + 0} = 1$\n",
    "\n",
    "Thus, the required probability is $1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FYI, a more updated and correct version of the code given below is in Assignment3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.7878\n",
      "accuracy: 0.8082\n",
      "accuracy: 0.7796\n",
      "accuracy: 0.7694\n",
      "accuracy: 0.7755\n",
      "accuracy: 0.7857\n",
      "accuracy: 0.7939\n",
      "accuracy: 0.7714\n",
      "accuracy: 0.7812\n",
      "accuracy: 0.7832\n",
      "final accuracy: 0.7836\n"
     ]
    }
   ],
   "source": [
    "# FoML Assign 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "\n",
    "import numpy as np\n",
    "import csv \n",
    "import math \n",
    "MAX_DEPTH = 100\n",
    "# Enter You Name Here\n",
    "myname = \"Tanmay-Goyal\" # or \"Amar-Akbar-Antony\"\n",
    "\n",
    "# Implement your decision tree below\n",
    "#class DecisionTree:\n",
    "class DecisionNode:\n",
    "\n",
    "    def __init__(self,depth):\n",
    "        self.is_terminal_node = False\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.split_index = None\n",
    "        self.split_threshold = None\n",
    "        self.label = None\n",
    "        self.depth = depth\n",
    "        pass\n",
    "\n",
    "    def train(self,X,y):\n",
    "        # self.label = majority label of training data\n",
    "        \n",
    "        ones = np.count_nonzero(y==1)\n",
    "        zeros = np.count_nonzero(y==0)\n",
    "        if ones>zeros:\n",
    "            self.label = 1\n",
    "        else:\n",
    "            self.label = 0 \n",
    "        # If max depth reached, just stop ->is_terminal_node = True\n",
    "        if self.depth == MAX_DEPTH:\n",
    "            is_terminal_node = True\n",
    "        # Calculate initial entropy\n",
    "        entropy_init = entropy(y)\n",
    "        \n",
    "        # If initial entropy>0.1 -> Find your best split -> splitindex,threshold \n",
    "        if entropy_init>0.1:\n",
    "            entropy_max = 100\n",
    "            # looping over all attributes\n",
    "            X_yes_final = []\n",
    "            y_yes_final = []\n",
    "            X_no_final  = []\n",
    "            y_no_final = []\n",
    "            for column in range(len(X[0])):\n",
    "                maxi = np.max(X[: , column])\n",
    "                mini = np.min(X[: , column])    \n",
    "                potential_thresholds = np.linspace(mini, maxi , 20)\n",
    "                # looping over all thresholds\n",
    "                for t in potential_thresholds:\n",
    "                    yes = np.where(X[: , column]>=t)[0] # greater than threshold\n",
    "                    no = np.where(X[: , column]<t)[0]   # lesser than threshold\n",
    "                    if len(yes)==0 or len(no)==0:\n",
    "                        continue\n",
    "                    \n",
    "                    y_yes = np.array([y[i] for i in yes])\n",
    "                    y_no = np.array([y[i] for i in no])\n",
    "                    total = len(y_yes) + len(y_no)\n",
    "                    entropy_yes = entropy(y_yes)\n",
    "                    entropy_no = entropy(y_no)\n",
    "                    \n",
    "                    entropy_final = (len(y_yes)/total)*entropy_yes + (len(y_no)/total)*entropy_no\n",
    "                    # maximising information gain -> minimising entropy\n",
    "                    if entropy_final<entropy_max:\n",
    "                            self.split_index = column\n",
    "                            self.split_threshold = t\n",
    "                            entropy_max = entropy_final\n",
    "                            X_yes_final = np.array([X[i] for i in yes])\n",
    "                            X_no_final = np.array([X[i] for i in no] )  \n",
    "                            y_yes_final = np.array([y[i] for i in yes])\n",
    "                            y_no_final = np.array([y[i] for i in no])\n",
    "                            \n",
    "        \n",
    "            # assuming left path corresponds to data greater than threshold\n",
    "            l = DecisionNode(self.depth+1)\n",
    "            self.left = l\n",
    "            r = DecisionNode(self.depth+1)\n",
    "            self.right = r\n",
    "            \n",
    "            l.train(X_yes_final , y_yes_final)\n",
    "            r.train(X_no_final , y_no_final)\n",
    "        \n",
    "        else:\n",
    "            self.is_terminal_node = True\n",
    "\n",
    "    # implement this function\n",
    "    def classify(self, test_instance):\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "        \n",
    "        if(self.is_terminal_node==True):\n",
    "            return self.label\n",
    "        else:\n",
    "            if test_instance[self.split_index]>=self.split_threshold:\n",
    "                self.left.classify(test_instance)\n",
    "            else:\n",
    "                self.right.classify(test_instance)\n",
    "                \n",
    "        return result\n",
    "\n",
    "def entropy(y):\n",
    "    ones = np.count_nonzero(y==1)/len(y)\n",
    "\n",
    "    zeros = np.count_nonzero(y==0)/len(y)\n",
    "    \n",
    "    if ones==0 or zeros==0:\n",
    "        entropy = 0\n",
    "    else:\n",
    "        entropy = ones*math.log2(ones) + zeros * math.log2(zeros)\n",
    "    return -entropy\n",
    "    \n",
    "def run_decision_tree():\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = np.array([tuple(line) for line in csv.reader(f, delimiter=\",\")]).astype(float)\n",
    "    print (\"Number of records: %d\" % len(data))\n",
    "\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    sum_accuracy = 0\n",
    "    K = 10\n",
    "    for j in range(K):\n",
    "        training_set = np.array([x for i, x in enumerate(data) if i % K != j])\n",
    "        test_set = np.array([x for i, x in enumerate(data) if i % K == j])\n",
    "    \n",
    "        train_x =training_set[:,:-1]\n",
    "        train_y = training_set[:,-1].astype(int)\n",
    "        test_x = test_set[: , :-1]\n",
    "        test_y = test_set[: , -1].astype(int)\n",
    "\n",
    "        \n",
    "        tree = DecisionNode(0)\n",
    "        # Construct a tree using training set\n",
    "        \n",
    "        tree.train( train_x , train_y )\n",
    "        #print(\"trained\" , j)\n",
    "    \n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = tree.classify( instance[:-1] )\n",
    "            results.append( result == instance[-1])\n",
    "        #print(results)\n",
    "        \n",
    "        # Accuracy\n",
    "        \n",
    "        accuracy = float(results.count(True))/float(len(results))\n",
    "        print (\"accuracy: %.4f\" % accuracy)\n",
    "        sum_accuracy+=accuracy\n",
    "        \n",
    "        \n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % (sum_accuracy/K))\n",
    "    f.close()\n",
    "    print(\"final accuracy: %.4f\" % (sum_accuracy/K))\n",
    "    \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving the Decision Tree (Gini Index)\n",
    "\n",
    "Gini Index usually lies in the range of $(0,0.5)$ while entropy lies between $0$ and $1$. Thus, Gini Index tends to penalise impurities lesser than what entropy does. Also, Gini Index is computationally better since it does not require computing logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.7878\n",
      "accuracy: 0.8082\n",
      "accuracy: 0.7796\n",
      "accuracy: 0.7694\n",
      "accuracy: 0.7755\n",
      "accuracy: 0.7857\n",
      "accuracy: 0.7939\n",
      "accuracy: 0.7714\n",
      "accuracy: 0.7812\n",
      "accuracy: 0.7832\n",
      "final accuracy: 0.7836\n"
     ]
    }
   ],
   "source": [
    "# FoML Assign 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "\n",
    "import numpy as np\n",
    "import csv \n",
    "import math \n",
    "MAX_DEPTH = 100\n",
    "# Enter You Name Here\n",
    "myname = \"Tanmay-Goyal\" # or \"Amar-Akbar-Antony\"\n",
    "\n",
    "# Implement your decision tree below\n",
    "#class DecisionTree:\n",
    "class DecisionNode:\n",
    "\n",
    "    def __init__(self,depth):\n",
    "        self.is_terminal_node = False\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.split_index = None\n",
    "        self.split_threshold = None\n",
    "        self.label = None\n",
    "        self.depth = depth\n",
    "        pass\n",
    "\n",
    "    def train(self,X,y):\n",
    "        # self.label = majority label of training data\n",
    "        \n",
    "        ones = np.count_nonzero(y==1)\n",
    "        zeros = np.count_nonzero(y==0)\n",
    "        if ones>zeros:\n",
    "            self.label = 1\n",
    "        else:\n",
    "            self.label = 0 \n",
    "        # If max depth reached, just stop ->is_terminal_node = True\n",
    "        if self.depth == MAX_DEPTH:\n",
    "            is_terminal_node = True\n",
    "        # Calculate initial entropy\n",
    "        entropy_init = gini_index(y)\n",
    "        \n",
    "        \n",
    "        # If initial entropy>0.1 -> Find your best split -> splitindex,threshold \n",
    "        if entropy_init>0.1:\n",
    "            entropy_max = 100\n",
    "            # looping over all attributes\n",
    "            X_yes_final = []\n",
    "            y_yes_final = []\n",
    "            X_no_final  = []\n",
    "            y_no_final = []\n",
    "            for column in range(len(X[0])):\n",
    "                maxi = np.max(X[: , column])\n",
    "                mini = np.min(X[: , column])    \n",
    "                potential_thresholds = np.linspace(mini, maxi , 20)\n",
    "                # looping over all thresholds\n",
    "                for t in potential_thresholds:\n",
    "                    yes = np.where(X[: , column]>=t)[0] # greater than threshold\n",
    "                    no = np.where(X[: , column]<t)[0]   # lesser than threshold\n",
    "                    if len(yes)==0 or len(no)==0:\n",
    "                        continue\n",
    "                    \n",
    "                    y_yes = np.array([y[i] for i in yes])\n",
    "                    y_no = np.array([y[i] for i in no])\n",
    "                    total = len(y_yes) + len(y_no)\n",
    "                    entropy_yes = gini_index(y_yes)\n",
    "                    entropy_no = gini_index(y_no)\n",
    "                    \n",
    "                    entropy_final = (len(y_yes)/total)*entropy_yes + (len(y_no)/total)*entropy_no\n",
    "                    # maximising information gain -> minimising entropy\n",
    "                    if entropy_final<entropy_max:\n",
    "                            self.split_index = column\n",
    "                            self.split_threshold = t\n",
    "                            entropy_max = entropy_final\n",
    "                            X_yes_final = np.array([X[i] for i in yes])\n",
    "                            X_no_final = np.array([X[i] for i in no] )  \n",
    "                            y_yes_final = np.array([y[i] for i in yes])\n",
    "                            y_no_final = np.array([y[i] for i in no])\n",
    "                            \n",
    "        \n",
    "            # assuming left path corresponds to data greater than threshold\n",
    "            l = DecisionNode(self.depth+1)\n",
    "            self.left = l\n",
    "            r = DecisionNode(self.depth+1)\n",
    "            self.right = r\n",
    "            \n",
    "            l.train(X_yes_final , y_yes_final)\n",
    "            r.train(X_no_final , y_no_final)\n",
    "        \n",
    "        else:\n",
    "            self.is_terminal_node = True\n",
    "\n",
    "    # implement this function\n",
    "    def classify(self, test_instance):\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "    \n",
    "        if(self.is_terminal_node==True):\n",
    "            return self.label\n",
    "        else:\n",
    "            if test_instance[self.split_index]>=self.split_threshold:\n",
    "                self.left.classify(test_instance)\n",
    "            else:\n",
    "                self.right.classify(test_instance)\n",
    "            \n",
    "        return result\n",
    "\n",
    "def gini_index(y):\n",
    "    ones = np.count_nonzero(y==1)/len(y)\n",
    "\n",
    "    zeros = np.count_nonzero(y==0)/len(y)\n",
    "    \n",
    "    return 1 - (ones**2) - (zeros**2)\n",
    "    \n",
    "def run_decision_tree():\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = np.array([tuple(line) for line in csv.reader(f, delimiter=\",\")]).astype(float)\n",
    "    print (\"Number of records: %d\" % len(data))\n",
    "\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    sum_accuracy = 0\n",
    "    K = 10\n",
    "    for j in range(K):\n",
    "        training_set = np.array([x for i, x in enumerate(data) if i % K != j])\n",
    "        test_set = np.array([x for i, x in enumerate(data) if i % K == j])\n",
    "    \n",
    "        train_x =training_set[:,:-1]\n",
    "        train_y = training_set[:,-1].astype(int)\n",
    "        test_x = test_set[: , :-1]\n",
    "        test_y = test_set[: , -1].astype(int)\n",
    "\n",
    "        \n",
    "        tree = DecisionNode(0)\n",
    "        # Construct a tree using training set\n",
    "        \n",
    "        tree.train( train_x , train_y )\n",
    "        #print(\"trained\" , j)\n",
    "    \n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = tree.classify( instance[:-1] )\n",
    "            results.append( result == instance[-1])\n",
    "        #print(results)\n",
    "        \n",
    "        # Accuracy\n",
    "        \n",
    "        accuracy = float(results.count(True))/float(len(results))\n",
    "        print (\"accuracy: %.4f\" % accuracy)\n",
    "        sum_accuracy+=accuracy\n",
    "        \n",
    "        \n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"a\")\n",
    "    f.write(\"\\n Improvement using Gini Index\")\n",
    "    f.write(\"\\n accuracy: %.4f\" % (sum_accuracy/K))\n",
    "    f.close()\n",
    "    print(\"final accuracy: %.4f\" % (sum_accuracy/K))\n",
    "    \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving the Decision Tree (multi-way split)\n",
    "Multiway splits have the advantage that an attribute closer to the root node would be very rarely repeated down the tree. This makes the trees more easily comprehensible and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.7878\n",
      "accuracy: 0.8082\n",
      "accuracy: 0.7796\n",
      "accuracy: 0.7694\n",
      "accuracy: 0.7755\n",
      "accuracy: 0.7857\n",
      "accuracy: 0.7939\n",
      "accuracy: 0.7714\n",
      "accuracy: 0.7812\n",
      "accuracy: 0.7832\n",
      "final accuracy: 0.7836\n"
     ]
    }
   ],
   "source": [
    "# FoML Assign 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "\n",
    "import numpy as np\n",
    "import csv \n",
    "import math \n",
    "MAX_DEPTH = 100\n",
    "# Enter You Name Here\n",
    "myname = \"Tanmay-Goyal\" # or \"Amar-Akbar-Antony\"\n",
    "\n",
    "# Implement your decision tree below\n",
    "#class DecisionTree:\n",
    "class DecisionNode:\n",
    "\n",
    "    def __init__(self,depth):\n",
    "        self.is_terminal_node = False\n",
    "        #taking 4 splits per node\n",
    "        self.child1 = None\n",
    "        self.child2 = None\n",
    "        self.child3 = None\n",
    "        self.child4 = None\n",
    "        self.split_index = None\n",
    "        self.split_threshold = None\n",
    "        self.label = None\n",
    "        self.depth = depth\n",
    "        pass\n",
    "\n",
    "    def train(self,X,y):\n",
    "        # self.label = majority label of training data\n",
    "        \n",
    "        ones = np.count_nonzero(y==1)\n",
    "        zeros = np.count_nonzero(y==0)\n",
    "        if ones>zeros:\n",
    "            self.label = 1\n",
    "        else:\n",
    "            self.label = 0 \n",
    "        # If max depth reached, just stop ->is_terminal_node = True\n",
    "        if self.depth == MAX_DEPTH:\n",
    "            is_terminal_node = True\n",
    "        # Calculate initial entropy\n",
    "        entropy_init = entropy(y)\n",
    "        \n",
    "        # If initial entropy>0.1 -> Find your best split -> splitindex,threshold \n",
    "        if entropy_init>0.1:\n",
    "            \n",
    "            entropy_max = 100\n",
    "            # looping over all attributes\n",
    "            Xfinal = []\n",
    "            yfinal = []\n",
    "            entropy_sum = 0\n",
    "            for column in range(len(X[0])):\n",
    "                maxi = np.max(X[: , column])\n",
    "                mini = np.min(X[: , column])    \n",
    "                potential_thresholds = np.linspace(mini, maxi , 5)\n",
    "                # looping over all thresholds\n",
    "                for t in range(len(potential_thresholds)-1):\n",
    "                        yes = np.where((X[: , column]>=potential_thresholds[t]) & (X[: , column]<potential_thresholds[t+1]))\n",
    "                        y_yes =  np.array([y[i] for i in yes])\n",
    "                        entropy_yes = entropy(y_yes)\n",
    "                        entropy_sum += (len(yes)/len(y))*entropy_yes\n",
    "                \n",
    "                    # maximising information gain -> minimising entropy\n",
    "                    \n",
    "                if entropy_sum<entropy_max:\n",
    "                            self.split_index = column\n",
    "                            self.split_threshold = potential_thresholds\n",
    "                            entropy_max = entropy_sum\n",
    "                            for t in range(len(potential_thresholds)-1):\n",
    "                                yes = np.where((X[: , column]>=potential_thresholds[t]) & (X[: , column]<potential_thresholds[t+1]))\n",
    "                                y_yes =  np.array([y[i] for i in yes])\n",
    "                                Xfinal.append(np.array([X[i] for i in yes]))\n",
    "                                yfinal.append(np.array([y[i] for i in yes]))\n",
    "                            \n",
    "            c1 = DecisionNode(self.depth+1)\n",
    "            self.child1 = c1\n",
    "            c2 = DecisionNode(self.depth+1)\n",
    "            self.child2 = c2\n",
    "            c3 = DecisionNode(self.depth+1)\n",
    "            self.child3 = c3\n",
    "            c4 = DecisionNode(self.depth+1)\n",
    "            self.child4 = c4\n",
    "            \n",
    "            c1.train(Xfinal[0] , yfinal[0])\n",
    "            c2.train(Xfinal[1] , yfinal[1])\n",
    "            c3.train(Xfinal[2] , yfinal[2])\n",
    "            c4.train(Xfinal[3] , yfinal[3])\n",
    "        \n",
    "        else:\n",
    "            self.is_terminal_node = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    # implement this function\n",
    "    def classify(self, test_instance):\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "        \n",
    "        if(self.is_terminal_node==True):\n",
    "            return self.label\n",
    "        else:\n",
    "            if test_instance[self.split_index]>=self.split_threshold[0] and test_instance[self.split_index]<self.split_threshold[1]:\n",
    "                self.child1.classify(test_instance)\n",
    "            elif test_instance[self.split_index]>=self.split_threshold[1] and test_instance[self.split_index]<self.split_threshold[2]:\n",
    "                self.child2.classify(test_instance)\n",
    "            elif test_instance[self.split_index]>=self.split_threshold[2] and test_instance[self.split_index]<self.split_threshold[3]:\n",
    "                self.child3.classify(test_instance)\n",
    "            else:\n",
    "                self.child4.classify(test_instance)\n",
    "        return result\n",
    "\n",
    "def entropy(y):\n",
    "    ones = np.count_nonzero(y==1)/len(y)\n",
    "\n",
    "    zeros = np.count_nonzero(y==0)/len(y)\n",
    "    \n",
    "    if ones==0 or zeros==0:\n",
    "        entropy = 0\n",
    "    else:\n",
    "        entropy = ones*math.log2(ones) + zeros * math.log2(zeros)\n",
    "    return -entropy\n",
    "    \n",
    "def run_decision_tree():\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = np.array([tuple(line) for line in csv.reader(f, delimiter=\",\")]).astype(float)\n",
    "    print (\"Number of records: %d\" % len(data))\n",
    "\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    sum_accuracy = 0\n",
    "    K = 10\n",
    "    for j in range(K):\n",
    "        training_set = np.array([x for i, x in enumerate(data) if i % K != j])\n",
    "        test_set = np.array([x for i, x in enumerate(data) if i % K == j])\n",
    "    \n",
    "        train_x =training_set[:,:-1]\n",
    "        train_y = training_set[:,-1].astype(int)\n",
    "        test_x = test_set[: , :-1]\n",
    "        test_y = test_set[: , -1].astype(int)\n",
    "\n",
    "        \n",
    "        tree = DecisionNode(0)\n",
    "        # Construct a tree using training set\n",
    "        \n",
    "        tree.train( train_x , train_y )\n",
    "        #print(\"trained\" , j)\n",
    "    \n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = tree.classify( instance[:-1] )\n",
    "            results.append( result == instance[-1])\n",
    "        #print(results)\n",
    "        \n",
    "        # Accuracy\n",
    "        \n",
    "        accuracy = float(results.count(True))/float(len(results))\n",
    "        print (\"accuracy: %.4f\" % accuracy)\n",
    "        sum_accuracy+=accuracy\n",
    "        \n",
    "        \n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"a\")\n",
    "    f.write(\"\\n Improvement using Multi-way split\")\n",
    "    f.write(\"\\n accuracy: %.4f\" % (sum_accuracy/K))\n",
    "    f.close()\n",
    "    print(\"final accuracy: %.4f\" % (sum_accuracy/K))\n",
    "    \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bec9f652a4082306f869e119250ed899f786ad91c86aea518642b2537134d841"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
