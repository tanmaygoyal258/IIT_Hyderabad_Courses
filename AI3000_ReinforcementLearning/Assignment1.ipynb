{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI3000: Reinforcement Learning\n",
    "\n",
    "#### Assignment 1\n",
    "\n",
    "#### Tanmay Goyal\n",
    "\n",
    "#### AI20BTECH11021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Any required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 1: Markov Reward Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>States</u></b> : {$S , 1 , 3 , 5 , 6 , 7 , 8 , W$}\n",
    "\n",
    "$2,4,9$ are not considered states because whenever we land at these states, there is no possibile actions from these states because we will immediately transition to another state, using the snakes or ladders.\n",
    "\n",
    "<b><u>Transition Matrix</u></b> : Since the die is a fair sided 4 faced die, the probabiity of moving to any of the next 4 states is 0.25.\n",
    "\n",
    "$\\mathcal{P} = \\begin{bmatrix}\n",
    "0 & 0.25 & 0.25 & 0 & 0 & 0.25 & 0.25 & 0\\\\ %S\n",
    "0 & 0 & 0.25 & 0.25 & 0 & 0.25 & 0.25 & 0\\\\ %1\n",
    "0 & 0 & 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0\\\\ %3\n",
    "0 & 0 & 0.25 & 0 & 0.25 & 0.25 & 0.25 & 0\\\\ %5\n",
    "0 & 0 & 0.25 & 0 & 0 & 0.25 & 0.25 & 0.25\\\\ %6\n",
    "0 & 0 & 0.25 & 0 & 0 & 0.25 & 0.25 & 0.25\\\\ %7\n",
    "0 & 0 & 0.25 & 0 & 0 & 0 & 0.50 & 0.25\\\\ %8\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\ %W\n",
    "\\end{bmatrix}$\n",
    "\n",
    "<b><u>Reward Function, Discount factor and Values</u></b> : The reward function can be constructed as follows:\n",
    "\n",
    "$\\mathcal{R} = \\begin{bmatrix}\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "We construct this reward function because we want to minimize the number of steps that the agent takes to reach the goal state. Every non-terminal state will result in a negative reward, and it will want to minimize that.   \n",
    "\n",
    "We choose $\\gamma = 1$ because we want a reduction in the number of steps, and also we want to keep an eye out for our future states and rewards as well. Moreover, since it would always result in a finite sequence of steps, we donot need to discount our rewards.\n",
    "\n",
    "We know, $\\mathcal{V} = (I - \\gamma\\mathcal{P})^{-1}\\mathcal{R}$\n",
    "\n",
    "Since, $I - \\gamma\\mathcal{P}$ is singular when $\\gamma = 1$, we take a limit by setting $\\gamma = 0.999999$.\n",
    "\n",
    "We get, \n",
    "\n",
    "$\\mathcal{V} = \\begin{bmatrix}\n",
    "-7.0833\\\\\n",
    "-7\\\\\n",
    "-6.666\\\\\n",
    "-6.666\\\\\n",
    "-5.333\\\\\n",
    "-5.333\\\\\n",
    "-5.333\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "The code for the computation of $\\mathcal{V}$ is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value function is : \n",
      " [-7.08329803 -6.99996533 -6.66663422 -6.66663422 -5.33330844 -5.33330844\n",
      " -5.33330844  0.        ]\n"
     ]
    }
   ],
   "source": [
    "I = np.identity(8)\n",
    "\n",
    "P = np.array([\n",
    "        [0 , 0.25 , 0.25 , 0 , 0 , 0.25 , 0.25 , 0],\n",
    "        [0 , 0 , 0.25 , 0.25 , 0 , 0.25 , 0.25 , 0],\n",
    "        [0 , 0 , 0 , 0.25 , 0.25 , 0.25 , 0.25 , 0],\n",
    "        [0 , 0 , 0.25 , 0 , 0.25 , 0.25 , 0.25 , 0],\n",
    "        [0 , 0 , 0.25 , 0 , 0, 0.25 , 0.25 , 0.25],\n",
    "        [0 , 0 , 0.25 , 0 , 0, 0.25 , 0.25 , 0.25],\n",
    "        [0 , 0 , 0.25 , 0 , 0, 0 , 0.50 , 0.25],\n",
    "        [0 , 0 , 0 , 0 , 0 ,0 , 0 , 1]\n",
    "])\n",
    "\n",
    "R = np.array([-1 , -1 , -1 , -1 , -1 , -1 , -1 , 0]).T\n",
    "\n",
    "gamma = 0.999999\n",
    "\n",
    "print(\"The value function is : \\n\" , np.linalg.inv(I - gamma*P)  @ R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 2: Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Enumerate the state space, action space, reward function and transition matrix.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "The state space $S = \\{0 , 1 , 2 , 3 , \\ldots , N \\}$ which represent the number of working machines on a particular day. Note that $|S| = N+1$\n",
    "\n",
    "The action space $A$ consists of two possible actions: $a_1$ which involves not calling the repair man on a particular day, and $a_2$, which involves calling the repair man on a particular day.\n",
    "\n",
    "The transition matrix for action $a_1$ will look like:\n",
    "\n",
    "$\\mathcal{P}^{a_1}_{ss'} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & \\ldots & 0 & 0\\\\\n",
    "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & \\ldots & 0 & 0\\\\\n",
    "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & \\ldots & 0 & 0\\\\\n",
    "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\ldots & 0 & 0\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{1}{N} & \\frac{1}{N} & \\frac{1}{N} & \\frac{1}{N} & \\ldots & \\frac{1}{N} & 0\\\\\n",
    "\\frac{1}{N+1} & \\frac{1}{N+1} & \\frac{1}{N+1} & \\frac{1}{N+1} & \\ldots & \\frac{1}{N+1} & \\frac{1}{N+1}\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This we get from the fact that if on a particular day, $k$ machines are working, and $N-k$ machines are not working, and if the repair man is not called, then the $N-k$ broken machines would still not work the next day, and the probability that $j$ machines start the next day where $0\\leq j \\leq k$ will be $\\frac{1}{k+1}$\n",
    "\n",
    "The reward matrix for action $a_1$ would look like:\n",
    "\n",
    "$\\mathcal{R}^{a_1}_{ss'} = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & \\ldots & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & \\ldots & 1 & 1\\\\\n",
    "2 & 2 & 2 & 2 & \\ldots & 2 & 2\\\\\n",
    "3 & 3 & 3 & 3 & \\ldots & 3 & 3\\\\\n",
    "\\vdots\\\\\n",
    "N-1 & N-1 & N-1 & N-1 & \\ldots & N-1 & N-1\\\\\n",
    "N & N & N & N & \\ldots & N & N\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This we get from the fact that if on a particular day, $k$ machines are working, and $N-k$ machines are not working, and if the repair man is not called, each of the $k$ working machines would generate a revenue of $\\$1$, resulting in a revenue of $\\$k$ for that day.\n",
    "\n",
    "The transition matrix for action $a_2$ will look like:\n",
    "\n",
    "$\\mathcal{P}^{a_2}_{ss'} = \\begin{bmatrix}\n",
    "0 & 0 & \\ldots & 0 & 0 & 1\\\\\n",
    " 0 & 0 & \\ldots & 0 & \\frac{1}{2} & \\frac{1}{2}\\\\\n",
    " 0 & 0 & \\ldots & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\\\\n",
    "\\vdots\\\\\n",
    "0 & \\frac{1}{N} & \\frac{1}{N} & \\ldots & \\frac{1}{N} & \\frac{1}{N}\\\\\n",
    " \\frac{1}{N+1} & \\frac{1}{N+1} & \\frac{1}{N+1} & \\ldots & \\frac{1}{N+1} & \\frac{1}{N+1}\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This we get from the fact that if on a particular day, $k$ machines are working, and $N-k$ machines are not working, and if the repair man is called, then the $N-k$ broken machines would guarantee work the next day, and the probability that $j$ machines start the next day where $0\\leq j \\leq k$ will be $\\frac{1}{k+1}$. So, we have that $N-k+j$ machines will work with a probability of $\\frac{1}{k+1}$\n",
    "\n",
    "The reward matrix for action $a_2$ would look like:\n",
    "\n",
    "$\\mathcal{R}^{a_2}_{ss'} = \\begin{bmatrix}\n",
    "-\\frac{N}{2} & -\\frac{N}{2} & -\\frac{N}{2} & -\\frac{N}{2} & \\ldots & -\\frac{N}{2} & -\\frac{N}{2}\\\\\n",
    "1-\\frac{N}{2} & 1-\\frac{N}{2} & 1-\\frac{N}{2} & 1-\\frac{N}{2} & \\ldots & 1-\\frac{N}{2} & 1-\\frac{N}{2}\\\\\n",
    "2-\\frac{N}{2} & 2-\\frac{N}{2} & 2-\\frac{N}{2} & 2-\\frac{N}{2} & \\ldots & 2-\\frac{N}{2} & 2-\\frac{N}{2}\\\\\n",
    "3-\\frac{N}{2} & 3-\\frac{N}{2} & 3-\\frac{N}{2} & 3-\\frac{N}{2} & \\ldots & 3-\\frac{N}{2} & 3-\\frac{N}{2}\\\\\n",
    "\\vdots\\\\\n",
    "N-1-\\frac{N}{2} & N-1-\\frac{N}{2} & N-1-\\frac{N}{2} & N-1-\\frac{N}{2} & \\ldots & N-1-\\frac{N}{2} & N-1-\\frac{N}{2}\\\\\n",
    "N-\\frac{N}{2} & N-\\frac{N}{2} & N-\\frac{N}{2} & N-\\frac{N}{2} & \\ldots & N-\\frac{N}{2} & N-\\frac{N}{2}\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This we get from the fact that if on a particular day, $k$ machines are working, and $N-k$ machines are not working, and if the repair man is called, the cost for calling the repair man will be $\\$ \\frac{N}{2}$, apart from which the $k$ working machines will generate a revenue of $\\$1$ each, giving a total reward of $\\$(k - \\frac{N}{2})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Will it be a discounted or undiscounted setting?\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "At every state, the manager can choose to call the repair man to repair the broken machines, or continue to work with the number of machines that are already working. In case the manager does decide to call the repair man, then it does become an infinite horizon problem, in which case we will have to discount the setting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Suppose the manager adopts the policy of not calling the repair man ever. Find the values of all states, assuming there are 5 machines in the factory.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "Let the policy $\\pi$ involve the manager never calling the repair man. Then, we can say under this policy $\\pi$, the probability of taking the action $a_1$ , $\\pi(a_1|s) = 1$ for all states $s \\in S$. Thus, we get the transition matrix as:\n",
    "\n",
    "$\\mathcal{P}^{a_1}_{ss'} =  \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0\\\\\n",
    "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 & 0\\\\\n",
    "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 & 0\\\\\n",
    "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & 0\\\\\n",
    "\\frac{1}{6} & \\frac{1}{6} & \\frac{1}{6} & \\frac{1}{6} & \\frac{1}{6} & \\frac{1}{6}\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\mathcal{R}^{a_1}_{ss'} = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1\\\\\n",
    "2 & 2 & 2 & 2 & 2 & 2\\\\\n",
    "3 & 3 & 3 & 3 & 3 & 3\\\\\n",
    "4 & 4 & 4 & 4 & 4 & 4\\\\\n",
    "5 & 5 & 5 & 5 & 5 & 5\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "We also know, the policy $\\pi$ involves only selecting action $a_1$ at any given state $s \\in S$, thus, we can say $\\pi(a_1|s) = 1$ for all $s \\in S$. We also assume the discount factor $\\gamma = 1$, since in this case, the problem converts to a finite horizon MDP.\n",
    "\n",
    "We know , \n",
    "\n",
    "$V^{\\pi}(s) = \\sum\\limits_{a \\in A} \\pi(a|s) \\sum\\limits_{s'}\\mathcal{P}^{a}_{ss'}[\\mathcal{R}^{a}_{ss'} + \\gamma V^{\\pi}(s')]$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$V^\\pi(0) = 0$\n",
    "\n",
    "$V^\\pi(1) = \\frac{1}{2}[1 + \\gamma V^{\\pi}(0)] + \\frac{1}{2}[1 + \\gamma V^{\\pi}(1)] = 1 + \\frac{1}{2}V^\\pi(1) \\implies V^\\pi(1) = 2$\n",
    "\n",
    "$V^\\pi(2) = \\frac{1}{3}[2 + \\gamma V^{\\pi}(0)] + \\frac{1}{3}[2 + \\gamma V^{\\pi}(1)] + \\frac{1}{3}[2 + \\gamma V^{\\pi}(2)] = 2 + \\frac{1}{3}V^\\pi(1) + \\frac{1}{3}V^\\pi(2) \\implies V^\\pi(2) = 4$\n",
    "\n",
    "$V^\\pi(3) = \\frac{1}{4}[3 + \\gamma V^{\\pi}(0)] + \\frac{1}{4}[3 + \\gamma V^{\\pi}(1)] + \\frac{1}{4}[3 + \\gamma V^{\\pi}(2)] + \\frac{1}{4}[3 + \\gamma V^{\\pi}(3)]= 3 + \\frac{1}{4}V^\\pi(1) + \\frac{1}{4}V^\\pi(2) + \\frac{1}{4}V^\\pi(3) \\implies V^\\pi(3) = 6$\n",
    "\n",
    "$V^\\pi(4) = \\frac{1}{5}[4 + \\gamma V^{\\pi}(0)] + \\frac{1}{5}[4 + \\gamma V^{\\pi}(1)] + \\frac{1}{5}[4 + \\gamma V^{\\pi}(2)] + \\frac{1}{5}[4 + \\gamma V^{\\pi}(3)] + \\frac{1}{5}[4 + \\gamma V^{\\pi}(4)]= 4 + \\frac{1}{5}V^\\pi(1) + \\frac{1}{5}V^\\pi(2) + \\frac{1}{5}V^\\pi(3) + \\frac{1}{5}V^\\pi(4) \\implies V^\\pi(4) = 8$\n",
    "\n",
    "$V^\\pi(5) = \\frac{1}{6}[5 + \\gamma V^{\\pi}(0)] + \\frac{1}{6}[5 + \\gamma V^{\\pi}(1)] + \\frac{1}{6}[5 + \\gamma V^{\\pi}(2)] + \\frac{1}{6}[5 + \\gamma V^{\\pi}(3)] + \\frac{1}{6}[5 + \\gamma V^{\\pi}(4)] + \\frac{1}{6}[5 + \\gamma V^{\\pi}(5)]= 5 + \\frac{1}{6}V^\\pi(1) + \\frac{1}{6}V^\\pi(2) + \\frac{1}{6}V^\\pi(3) + \\frac{1}{6}V^\\pi(4) + \\frac{1}{6}V^\\pi(5) \\implies V^\\pi(5) = 10$\n",
    "\n",
    "Thus, we get\n",
    "\n",
    "$V^\\pi = \\begin{bmatrix}\n",
    "0\\\\\n",
    "2\\\\\n",
    "4\\\\\n",
    "6\\\\\n",
    "8\\\\\n",
    "10\\\\\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Perform one iteration of policy iteration on the no-repair policy to get an updated policy.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "Policy iteration invloves two steps: evaluation of the policies, and improvement of the same.\n",
    "\n",
    "Let the initial policy of never calling a repair man be represented by $\\pi_0$. We have already evaluated the policy $\\pi_0$:\n",
    "\n",
    "$V^{\\pi_0} = \\begin{bmatrix}\n",
    "0\\\\\n",
    "2\\\\\n",
    "4\\\\\n",
    "6\\\\\n",
    "8\\\\\n",
    "10\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Now, we would improve the policy $\\pi_0$. We call the improved policy $\\pi_1$\n",
    "\n",
    "$\\pi_1 = greedy(V^{\\pi_0})$\n",
    "\n",
    "$greedy(V^{\\pi_0}) = \\begin{cases}\n",
    "1 & a = \\argmax\\limits_{a\\in A} Q^{\\pi_0}(s , a)\\\\\n",
    "0 & otherwise\n",
    "\\end{cases}$\n",
    "\n",
    "$Q^\\pi(s,a) = \\sum\\limits_{s'\\in S} \\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{ss'} + \\gamma V^{\\pi}(s)]$\n",
    "\n",
    "Since there exist two actions, $a_1$ for not calling the repair-man, and $a_2$ for calling the repair-man, we find the action-value functions for both of the actions.\n",
    "\n",
    "\n",
    "$Q^{\\pi_0}(s,a_1) = \\sum\\limits_{s'\\in S} \\mathcal{P}^{a_1}_{ss'}[\\mathcal{R}^{a_1}_{ss'} + \\gamma V^{\\pi_0}(s)]$\n",
    "\n",
    "$Q^{\\pi_0}(0,a_1) = 1[0 + V^\\pi_0(0)] = 0$\n",
    "\n",
    "$Q^{\\pi_0}(1,a_1) = \\frac{1}{2}[1 + V^\\pi_0(0)] + \\frac{1}{2}[1 + V^\\pi_0(1)] = 2$\n",
    "\n",
    "$Q^{\\pi_0}(2,a_1) = \\frac{1}{3}[2 + V^\\pi_0(0)] + \\frac{1}{3}[2 + V^\\pi_0(1)] + \\frac{1}{3}[2 + V^\\pi_0(2)] = 4$\n",
    "\n",
    "$Q^{\\pi_0}(3,a_1) = \\frac{1}{4}[3 + V^\\pi_0(0)] + \\frac{1}{4}[3 + V^\\pi_0(1)] + \\frac{1}{4}[3 + V^\\pi_0(2)] + \\frac{1}{4}[3 + V^\\pi_0(3)] = 6$\n",
    "\n",
    "$Q^{\\pi_0}(4,a_1) = \\frac{1}{5}[4 + V^\\pi_0(0)] + \\frac{1}{5}[4 + V^\\pi_0(1)] + \\frac{1}{5}[4 + V^\\pi_0(2)] + \\frac{1}{5}[4 + V^\\pi_0(3)] + \\frac{1}{5}[4 + V^\\pi_0(4)] = 8$\n",
    "\n",
    "$Q^{\\pi_0}(5,a_1) = \\frac{1}{6}[5 + V^\\pi_0(0)] + \\frac{1}{6}[5 + V^\\pi_0(1)] + \\frac{1}{6}[5 + V^\\pi_0(2)] + \\frac{1}{6}[5 + V^\\pi_0(3)] + \\frac{1}{6}[5 + V^\\pi_0(4)] +  \\frac{1}{6}[5 + V^\\pi_0(5)] = 10$\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$Q^{\\pi_0}(s,a_2) = \\sum\\limits_{s'\\in S} \\mathcal{P}^{a_2}_{ss'}[\\mathcal{R}^{a_2}_{ss'} + \\gamma V^{\\pi_0}(s)]$\n",
    "\n",
    "$Q^{\\pi_0}(0,a_2) = 1[-2.5 + V^\\pi_0(5)] = 7.5$\n",
    "\n",
    "$Q^{\\pi_0}(1,a_2) = \\frac{1}{2}[-1.5 + V^\\pi_0(5)] + \\frac{1}{2}[-1.5 + V^\\pi_0(4)] = 7.5$\n",
    "\n",
    "$Q^{\\pi_0}(2,a_2) = \\frac{1}{3}[-0.5 + V^\\pi_0(5)] + \\frac{1}{3}[-0.5 + V^\\pi_0(4)] + \\frac{1}{3}[-0.5 + V^\\pi_0(3)] = 7.5$\n",
    "\n",
    "$Q^{\\pi_0}(3,a_2) = \\frac{1}{4}[0.5 + V^\\pi_0(5)] + \\frac{1}{4}[0.5 + V^\\pi_0(4)] + \\frac{1}{4}[0.5 + V^\\pi_0(3)] + \\frac{1}{4}[0.5 + V^\\pi_0(2)] = 7.5$\n",
    "\n",
    "$Q^{\\pi_0}(4,a_2) = \\frac{1}{5}[1.5 + V^\\pi_0(5)] + \\frac{1}{5}[1.5 + V^\\pi_0(4)] + \\frac{1}{5}[1.5 + V^\\pi_0(3)] + \\frac{1}{5}[1.5 + V^\\pi_0(2)] + \\frac{1}{5}[1.5 + V^\\pi_0(1)] = 7.5$\n",
    "\n",
    "$Q^{\\pi_0}(5,a_2) = \\frac{1}{6}[2.5 + V^\\pi_0(5)] + \\frac{1}{6}[2.5 + V^\\pi_0(4)] + \\frac{1}{6}[2.5 + V^\\pi_0(3)] + \\frac{1}{6}[2.5 + V^\\pi_0(2)] + \\frac{1}{6}[2.5 + V^\\pi_0(4)] +  \\frac{1}{6}[2.5 + V^\\pi_0(0)] = 7.5$\n",
    "\n",
    "Thus, for the optimal policy we get:\n",
    "\n",
    "|   \t| $Q(s , a_1)$ \t| $Q(s , a_2)$ \t| $a_1$ \t| $a_2$ \t|\n",
    "|---\t|--------------\t|--------------\t|-------\t|-------\t|\n",
    "| 0 \t| 0            \t| 7.5          \t| 0     \t| 1     \t|\n",
    "| 1 \t| 2            \t| 7.5          \t| 0     \t| 1     \t|\n",
    "| 2 \t| 4            \t| 7.5          \t| 0     \t| 1     \t|\n",
    "| 3 \t| 6            \t| 7.5          \t| 0     \t| 1     \t|\n",
    "| 4 \t| 8            \t| 7.5          \t| 1     \t| 0     \t|\n",
    "| 5 \t| 10           \t| 7.5          \t| 1     \t| 0     \t|\n",
    "\n",
    "$\\pi_1 = \\begin{cases}\n",
    "a_1 & s \\in \\{4,5\\}\\\\\n",
    "a_2 && otherwise\\\\\n",
    "\\end{cases}$\n",
    "\n",
    "where $a_1$ involves not calling the repair-man and $a_2$ involves calling the repair-man."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 3: On Ordering of Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Calculating $V^\\pi (s)$ for all states $s \\in S$. NOte that it is a stochastic environment, where the intended action might succeed with probability 0.9 and fail with probability 0.1.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "We will assume $\\gamma = 1$ for the calculation of $V^\\pi(.)$ since there exists a finite sequence of states with which we can reach the terminal state, and thus, we donot need to discount the rewards.\n",
    "\n",
    "Also, we know \n",
    "\n",
    "$V^\\pi (s) = \\sum\\limits_a \\pi(a | s) \\sum\\limits_{s'} \\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{ss'} + \\gamma V^\\pi (s')]$\n",
    "\n",
    "We first define the transition matrix for action $a_1$. Note that on taking action $a_1$, we might succeed with probability 0.9 and fail with probability 0.1, in which case taking action $a_1$ leads to the outcome given by $a_2$. Thus, \n",
    "\n",
    "$\\mathcal{P}^{a_1}_{ss'} = \\begin{bmatrix}\n",
    "0 & 0.9 & 0.1 & 0\\\\\n",
    "0.1 & 0 & 0 & 0.9\\\\\n",
    "0.9 & 0 & 0 & 0.1\\\\\n",
    "0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Similarly, we define the transition matrix for action $a_2$. Note that on taking action $a_2$, we might succeed with probability 0.9 and fail with probability 0.1, in which case taking action $a_2$ leads to the outcome given by $a_1$. Thus, \n",
    "\n",
    "$\\mathcal{P}^{a_2}_{ss'} = \\begin{bmatrix}\n",
    "0 & 0.1 & 0.9 & 0\\\\\n",
    "0.9 & 0 & 0 & 0.1\\\\\n",
    "0.1 & 0 & 0 & 0.9\\\\\n",
    "0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "We also define the reward matrix under action $a_1$ as (Note that in case of failure of action $a_1$, it may result in action $a_2$):\n",
    "\n",
    "$\\mathcal{R}^{a_1}_{ss'} = \\begin{bmatrix}\n",
    "0 & -10 & -10 & 0\\\\\n",
    "-10 & 0 & 0 & 100\\\\\n",
    "-10 & 0 & 0 & 100\\\\\n",
    "0 & 0 & 0 & 100\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Similarly, we also define the reward matrix under action $a_2$ as (Note that in case of failure of action $a_2$, it may result in action $a_1$):\n",
    "\n",
    "$\\mathcal{R}^{a_2}_{ss'} = \\begin{bmatrix}\n",
    "0 & -10 & -10 & 0\\\\\n",
    "-10 & 0 & 0 & 100\\\\\n",
    "-10 & 0 & 0 & 100\\\\\n",
    "0 & 0 & 0 & 100\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "We consider policy $\\pi_1$ which involves choosing action $a_1$. Thus, we can say under policy $\\pi_1$ , $\\pi_1(a_1|s) = 1$ for all $s \\in S$, and $\\pi_1(a_2|s) = 0$ for all $s \\in S$.\n",
    "\n",
    "$V^{\\pi_1}(D) = 1 \\times 100 = 100$\n",
    "\n",
    "$V^{\\pi_1}(C) = 0.9(-10  + V^{\\pi_1}(A)) + 0.1(-10 + V^{\\pi_1}(D)) = 0.9V^{\\pi_1}(A)$\n",
    "\n",
    "$V^{\\pi_1}(B) = 0.1(-10  + V^{\\pi_1}(A)) + 0.9(-10 + V^{\\pi_1}(D)) = 0.1V^{\\pi_1}(A) + 80$\n",
    "\n",
    "$V^{\\pi_1}(A) = 0.9(-10  + V^{\\pi_1}(B)) + 0.1(-10  + V^{\\pi_1}(C)) = -10 + 0.9V^{\\pi_1}(B) + 0.1V^{\\pi_1}(C) = 62 + 0.18V^{\\pi_1}(A)$\n",
    "\n",
    "Solving for $V^{\\pi_1}(A)$ , we get: \n",
    "\n",
    "$V^{\\pi_1}(A) = 75.6 , V^{\\pi_1}(B) = 87.56 , V^{\\pi_1}(C) = 68.04 , V^{\\pi_1}(D) = 100$\n",
    "\n",
    "Now, consider policy $\\pi_2$ ,which involves choosing action $a_2$. Thus, we can say under policy $\\pi_2$ , $\\pi_2(a_2|s) = 1$ for all $s \\in S$, and $\\pi_2(a_1|s) = 0$ for all $s \\in S$.\n",
    "\n",
    "\n",
    "$V^{\\pi_2}(D) = 1 \\times 100 = 100$\n",
    "\n",
    "$V^{\\pi_2}(B) = 0.9(-10  + V^{\\pi_2}(A)) + 0.1(-10 + V^{\\pi_2}(D)) = 0.9V^{\\pi_2}(A)$\n",
    "\n",
    "$V^{\\pi_2}(C) = 0.1(-10  + V^{\\pi_2}(A)) + 0.9(-10 + V^{\\pi_2}(D)) = 0.1V^{\\pi_2}(A) + 80$\n",
    "\n",
    "$V^{\\pi_2}(A) = 0.9(-10  + V^{\\pi_2}(C)) + 0.1(-10  + V^{\\pi_2}(B)) = -10 + 0.9V^{\\pi_2}(C) + 0.1V^{\\pi_2}(B) = 62 + 0.18V^{\\pi_2}(A)$\n",
    "\n",
    "Solving for $V^{\\pi_2}(A)$ , we get: \n",
    "\n",
    "$V^{\\pi_2}(A) = 75.6 , V^{\\pi_2}(B) = 68.04 , V^{\\pi_2}(C) = 87.56 , V^{\\pi_2}(D) = 100$\n",
    "\n",
    "Now, consider policy $\\pi_3$, which is a stochastic policy. It involves choosing action $a_1$ at B with probability 1, choosing action $a_2$ at C with probability 1, and choosing $a_1$ with probability 0.4 and $a_2$ with probability 0.6 at A. Thus, we can say:\n",
    "\n",
    "$\\pi_3(a_1 | A) = 0.4 , \\pi_3(a_2|A) = 0.6 , \\pi_3(a_1|B) = 1 , \\pi_3(a_2|B) = 0 , \\pi_3(a_1|C) = 0 , \\pi_3(a_2|C) = 1$\n",
    "\n",
    "$V^{\\pi_3}(D) = 1 \\times 100 = 100$\n",
    "\n",
    "$V^{\\pi_3}(B) = 0.1(-10  + V^{\\pi_3}(A)) + 0.9(-10 + V^{\\pi_3}(D)) = 0.1V^{\\pi_3}(A) + 80$\n",
    "\n",
    "$V^{\\pi_3}(C) = 0.1(-10  + V^{\\pi_3}(A)) + 0.9(-10 + V^{\\pi_3}(D)) = 0.1V^{\\pi_3}(A) + 80$\n",
    "\n",
    "$V^{\\pi_3}(A) = 0.4[0.9(-10 + V^{\\pi_3}(B)) + 0.1(-10 + V^{\\pi_3}(C))] + 0.6[0.1(-10 + V^{\\pi_3}(B)) + 0.9(-10 + V^{\\pi_3}(C))]= -10 + 0.58V^{\\pi_3}(C) + 0.42V^{\\pi_3}(B) = 70 + 0.1V^{\\pi_3}(A)$\n",
    "\n",
    "Solving for $V^{\\pi_3}(A)$ , we get: \n",
    "\n",
    "$V^{\\pi_3}(A) = 77.78, V^{\\pi_3}(B) = 87.78 , V^{\\pi_3}(C) = 87.78 , V^{\\pi_3}(D) = 100$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Finding the best policy\n",
    "\n",
    "<u>Answer</u>\n",
    "\n",
    "| Policy \t| $\\pi_1$ \t| $\\pi_2$ \t| $\\pi_3$ \t|\n",
    "|--------\t|---------\t|---------\t|---------\t|\n",
    "| A      \t| 75.6    \t| 75.6    \t| 77.78   \t|\n",
    "| B      \t| 87.56   \t| 68.04   \t| 87.78   \t|\n",
    "| C      \t| 68.04   \t| 87.56   \t| 87.78   \t|\n",
    "| D      \t| 100     \t| 100     \t| 100     \t|\n",
    "\n",
    "From this, we can see $\\pi_3 \\geq \\pi_2$ for all $s \\in  S$ and  $\\pi_3 \\geq \\pi_1$ for all $s \\in  S$. Thus, $\\pi_3$ is the best policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Checking if all the policies are comparable\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "| Policy \t| $\\pi_1$ \t| $\\pi_2$ \t| $\\pi_3$ \t|\n",
    "|--------\t|---------\t|---------\t|---------\t|\n",
    "| A      \t| 75.6    \t| 75.6    \t| 77.78   \t|\n",
    "| B      \t| 87.56   \t| 68.04   \t| 87.78   \t|\n",
    "| C      \t| 68.04   \t| 87.56   \t| 87.78   \t|\n",
    "| D      \t| 100     \t| 100     \t| 100     \t|\n",
    "\n",
    "We can see $\\pi_3 \\geq \\pi_2$ for all $s \\in  S$ and  $\\pi_3 \\geq \\pi_1$ for all $s \\in  S$. Thus, $\\pi_3$ is comparable to the other policies, since the same inequality holds for all states.\n",
    "\n",
    "However, if we wish to compare policies $\\pi_1$ and $\\pi_2$, we see that $\\pi_1 \\geq \\pi_2$ for states A,B and D, but the inequality reverses for state C. Thus, the two are not comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Given two deterministic stationary policies $\\pi_1$ and $\\pi_2$, can we construct a policy $\\pi$ for a arbitrary MDP, which is better than policies $\\pi_1$ and $\\pi_2$.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "Yes, we can. We would check the value of the policy at every state, and choose the action corresponding to $\\pi_1$ if $V^{\\pi_1}(s) > V^{\\pi_2}(s)$ else we consider the action corresponding to policy $\\pi_2$. Note that this can only be done because the policies are deterministic, which means at every step, we are choosing one out of two possible actions(corresponding to $\\pi_1$ or $\\pi_2$). \n",
    "\n",
    "Mathematically, let $\\pi_1(s)$ be the action corresponding to $\\pi_1$ at every state $s \\in S$ and $\\pi_2(s)$ be the action corresponding to $\\pi_2$ at every state $s \\in S$. Then,\n",
    "\n",
    "$\\pi(s) = \\begin{cases}\n",
    "\\pi_1(s) && V^{\\pi_1}(s) > V^{\\pi_2}(s)\\\\\n",
    "\\pi_2(s) && otherwise\n",
    "\\end{cases}$\n",
    "\n",
    "where $V^{\\pi}(s) = \\sum\\limits_a \\pi(a | s) \\sum\\limits_{s'} [\\mathcal{R}^a_{ss'} + \\gamma V^\\pi (s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 4: Effect of Noise and Discounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <u><b>Low $\\gamma$ , zero noise:\n",
    "</b></u>\n",
    "\n",
    "In this case, agent is short-sighted, and better rewards it may get in the future are not considered. So, it will prefer the close exit. Also, since there is zero noise, the environment is deterministic, and hence, it will not randomly fall into the cliff. Since the riskier path is shorter, and there is no danger of falling into the cliff, it will prefer the <u>close exit, but risk the cliff </u>.\n",
    "\n",
    "\n",
    "2. <u><b>Low $\\gamma$ , high noise:\n",
    "</b></u>\n",
    "\n",
    "In this case, agent is short-sighted, and better rewards it may get in the future are not considered. So, it will prefer the close exit. Also, since the noise is high, there is a high possibility that the intended action does not result in the intended consequence (stochastic environment). As a result, even if it takes a step that does not make it fall into the cliff, there is a chance it falls into the cliff. Hence, it would not want to risk the clff. So, it will prefer the <u>close exit, and not risk the cliff </u>.\n",
    "\n",
    "3. <u><b>High $\\gamma$ , zero noise:\n",
    "</b></u>\n",
    "\n",
    "In this case, agent is far-sighted, so better rewards in the future that it may earn are also equally tempting. Thus, the distant exit also becomes a possibiility. Also, since there is zero noise, the environment is deterministic, and hence, it will not randomly fall into the cliff. Since the riskier path is shorter, and there is no danger of falling into the cliff, it will prefer the <u>distant exit, but risk the cliff </u>.\n",
    "\n",
    "\n",
    "4. <u><b>High $\\gamma$ , high noise:\n",
    "</b></u>\n",
    "\n",
    "In this case, agent is far-sighted, so better rewards in the future that it may earn are also equally tempting. Thus, the distant exit also becomes a possibiility. Also, since the noise is high, there is a high possibility that the intended action does not result in the intended consequence (stochastic environment). As a result, even if it takes a step that does not make it fall into the cliff, there is a chance it falls into the cliff. Hence, it would not want to risk the clff. So, it will prefer the <u>distant exit, and not risk the cliff </u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 5: Value Functions\n",
    "\n",
    "$M_1 = <\\mathcal{S} , \\mathcal{A} , \\mathcal{P} , \\mathcal{R_1} , \\gamma>$\n",
    "\n",
    "$M_2 = <\\mathcal{S} , \\mathcal{A} , \\mathcal{P} , \\mathcal{R_2} , \\gamma>$\n",
    "\n",
    "$M_3 = <\\mathcal{S} , \\mathcal{A} , \\mathcal{P} , \\mathcal{R_1 + R_2} , \\gamma>$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) For a fixed policy $\\pi$, two ction value functions $Q_1^\\pi(s,a)$ and $Q_2^\\pi(s,a)$ are given. Can we combine these two to find $Q_3^\\pi(s,a)$ for $M_3$.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "Yes, we can.\n",
    "\n",
    "Note that since the reward function differs in the MDP, the value functions will also differ.\n",
    "\n",
    "For $M_1$ , $V^\\pi_1(s) = \\sum\\limits_{a} \\pi(a|s) \\sum\\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{1ss'} + \\gamma V^\\pi_1(s')]$\n",
    "\n",
    "For $M_2$ , $V^\\pi_2(s) = \\sum\\limits_{a} \\pi(a|s) \\sum\\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{2ss'} + \\gamma V^\\pi_2(s')]$\n",
    "\n",
    "For $M_3$ , $V^\\pi_3(s) = \\sum\\limits_{a} \\pi(a|s) \\sum\\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{1ss'} + \\mathcal{R}^a_{2ss'} + \\gamma V^\\pi_3(s')]$\n",
    "\n",
    "\n",
    "On adding $V^\\pi_1(s)$ and $V^\\pi_2(s)$, we get\n",
    "\n",
    "$V^\\pi_1(s) + V^\\pi_2(s) = \\sum\\limits_{a} \\pi(a|s) \\sum\\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{1ss'} + \\mathcal{R}^a_{2ss'} + \\gamma (V^\\pi_1(s') + V^\\pi_2(s'))]$\n",
    "\n",
    "On comparing $V^\\pi_3(s)$ and $V^\\pi_1(s) + V^\\pi_2(s)$, we can easily see:\n",
    "\n",
    "$V^\\pi_3(s) = V^\\pi_1(s) + V^\\pi_2(s)$ for all $s \\in S$\n",
    "\n",
    "Now, \n",
    "\n",
    "$Q^\\pi_1(s,a) = \\sum\\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{1ss'} + \\gamma V^\\pi_1(s')]$\n",
    "\n",
    "$Q^\\pi_2(s,a) = \\sum\\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{2ss'} + \\gamma V^\\pi_2(s')]$\n",
    "\n",
    "$Q^\\pi_3(s,a) = \\sum\\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{1ss'} + \\mathcal{R}^a_{2ss'} + \\gamma V^\\pi_3(s')] = \\sum\\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{1ss'} + \\mathcal{R}^a_{2ss'} + \\gamma (V^\\pi_1(s') + V^\\pi_2(s'))]$\n",
    "\n",
    "$\\implies Q^\\pi_3(s,a) = Q^\\pi_1(s,a) + Q^\\pi_2(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) If we are given optimal policies $\\pi_1^*$ and $\\pi_2^*$ for the two MDPs, can we obtain optimal policy $\\pi_3^*$ corresponding to $M_3$.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "No, we cannot. This is because while we can obtain the action- value functions from the given action-value functions, obtaining the optimal policies from given optimal policies is not so straight forward. Since the reward function for $M_3$ is linear combination of the reward functions for $M_1$ and $M_2$, and the optimal policies involve using the $max$ function, which is non-linear in nature, we cannot obtain the optimal policy $\\pi_3^*$ in a straightforward manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Suppose $\\pi^*$ is an optimal policy for both $M_1$ and $M_2$, will it also be an optimal policy for $M_3$?\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "Yes, it will be. Let the value function corresponding to the optimal policy be represented by $V_*(s)$\n",
    "\n",
    "We have,\n",
    "\n",
    "$V_{1*}(s) = \\max\\limits_{a} Q_{1*}(s,a)$\n",
    "\n",
    "$V_{2*}(s) = \\max\\limits_{a} Q_{2*}(s,a)$\n",
    "\n",
    "$V_{3*}(s) =  V_{1*}(s) +  V_{2*}(s) = \\max\\limits_{a} Q_{1*}(s,a) + \\max\\limits_{a} Q_{2*}(s,a) \\geq \\max\\limits_{a} Q_{3*}(s,a) = \\max\\limits_{a} [Q_{1*}(s,a) + Q_{2*}(s,a)] = \\max\\limits_{a} Q_{3*}(s,a)$\n",
    "\n",
    "$\\implies V_{3*}(s) \\geq \\max\\limits_{a} Q_{3*}(s,a)$\n",
    "\n",
    "Also, we know \n",
    "\n",
    "$V(s) \\leq \\max\\limits_a Q(s,a) \\implies V_{3*}(s) \\leq \\max\\limits_{a} Q_{3*}(s,a)$\n",
    "\n",
    "since $Q(s,a)$ involves taking the action $a$, and then checking the value from the next state. So, when a maximum over $a$ is taken, the value of the state has to be lesser than or equal to the maximum that is obtained.\n",
    "\n",
    "Thus, we get\n",
    "\n",
    "$V_{3*}(s) = \\max\\limits_{a} Q_{3*}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Assume \n",
    "\n",
    "$\\mathcal(R)_1(s,a,s') - \\mathcal{R}_2(s,a,s') = \\epsilon$\n",
    "\n",
    "Let the value function for a policy $\\pi$ for $M_1$ be $V^\\pi_1(s)$ and for $M_2$ be $V^\\pi_2(s)$. Find the relation between $V^\\pi_1(s)$ and $V^\\pi_2(s)$.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "We have,\n",
    "\n",
    "$V^\\pi_1(s) = \\sum\\limits_{a} \\pi(a|s) \\sum \\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{1ss'} + \\gamma V^\\pi_1(s')]$\n",
    "\n",
    "$V^\\pi_2(s) = \\sum\\limits_{a} \\pi(a|s) \\sum \\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{2ss'} + \\gamma V^\\pi_2(s')]$\n",
    "\n",
    "$\\implies V^\\pi_1(s) - V^\\pi_2(s) = \\sum\\limits_{a} \\pi(a|s) \\sum \\limits_{s'}\\mathcal{P}^a_{ss'}[\\mathcal{R}^a_{1ss'} - \\mathcal{R}^a_{2ss'} + \\gamma (V^\\pi_1(s') - V^\\pi_2(s'))]$\n",
    "\n",
    "\n",
    "A convenient assumption would be to let $V^\\pi_1(s) - V^\\pi_2(s) = \\beta$ for all $s \\in S$ where $\\beta$ is a constant.\n",
    "\n",
    "Thus, we can say,\n",
    "\n",
    "$\\beta = \\sum\\limits_{a} \\pi(a|s) \\sum \\limits_{s'}\\mathcal{P}^a_{ss'}[\\epsilon + \\gamma\\beta]$\n",
    "\n",
    "$\\implies \\beta = (\\epsilon + \\gamma\\beta)\\sum\\limits_{a} \\pi(a|s) \\sum \\limits_{s'}\\mathcal{P}^a_{ss'}$\n",
    "\n",
    "Note that,  $\\sum \\limits_{s'}\\mathcal{P}^a_{ss'} = 1$ and subsequently, $\\sum\\limits_{a} \\pi(a|s) = 1$.\n",
    "\n",
    "Thus,\n",
    "\n",
    "$\\beta = (\\epsilon + \\gamma\\beta)$\n",
    "\n",
    "$\\implies \\beta = \\frac{\\epsilon}{1 - \\gamma}$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$V^\\pi_1(s) - V^\\pi_2(s) =\\frac{\\epsilon}{1 - \\gamma}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0b3843ef66ef72e0d095359ab408deb80cd37476039c722f337af7e4c5c1d89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
