{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI3000: Reinforcement Learning\n",
    "\n",
    "#### Assignment 2\n",
    "\n",
    "#### Tanmay Goyal\n",
    "\n",
    "#### AI20BTECH11021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 1 Value Functions\n",
    "\n",
    "Let the true Value function $V^\\pi = (I - \\gamma P)^{-1}R$. We let the value iteration algorithm terminate when $||V_{k+1} - V_k||_\\infty \\leq \\epsilon$ for a given $\\epsilon > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Prove that the error estimate between obtained value function estimate $V_{k+1}$ and true error function $V^\\pi$ be given by \n",
    "\n",
    "$||V_{k+1} - V^\\pi||_\\infty \\leq \\frac{\\epsilon\\gamma}{1-\\gamma}$\n",
    "\n",
    "<u>Answer:</u>\n",
    "\n",
    "We define the Bellman Evaluation Operator as $L^\\pi(v) = R^\\pi + \\gamma P^\\pi v $ where:\n",
    "\n",
    "$R^\\pi = \\sum\\limits_a \\pi(a|s)\\sum\\limits_{s'}P^a_{ss'}R^a_{ss'}$\n",
    "\n",
    "$P^\\pi = \\sum\\limits\\pi(a|s)P^a_{ss'}$\n",
    "\n",
    "Then, we know, \n",
    "\n",
    "$L^\\pi(V_k) = V_{k+1}$\n",
    "\n",
    "$L^\\pi(V^\\pi) = V^\\pi$ where $V^\\pi$ is a fixed point.\n",
    "\n",
    "Thus, we can write\n",
    "\n",
    "$||V_{k+1} - V^\\pi||_\\infty = ||L^\\pi(V_{k}) - L^\\pi(V^\\pi)|| \\leq \\gamma ||V_k - V^\\pi||_\\infty$\n",
    "\n",
    "using the fact that the Bellman Evaluation Operator is a $\\gamma-$ contraction on the space of all value functions. We now wish to find a bound on $||V_k - V^\\pi||_\\infty$.\n",
    "\n",
    "$||V_k - V^\\pi||_\\infty = ||V_k - V_{k+1} + V_{k+1} - V^\\pi||_\\infty$\n",
    "\n",
    "$\\implies||V_k - V^\\pi||_\\infty \\leq ||V_k - V_{k+1}||_\\infty +  ||V_{k+1} - V^\\pi||_\\infty = \\epsilon + ||V_{k+1} - V^\\pi||_\\infty $ \n",
    "\n",
    "using the triangular inequality\n",
    "\n",
    "$\\implies||V_k - V^\\pi||_\\infty \\leq \\epsilon + ||L^\\pi(V_{k}) - L^\\pi(V^\\pi)|| \\leq \\epsilon + \\gamma ||V_k - V^\\pi||_\\infty$\n",
    "\n",
    "$\\implies ||V_k - V^\\pi||_\\infty \\leq \\frac{\\epsilon}{1-\\gamma} $\n",
    "\n",
    "Thus, we get\n",
    "\n",
    "$||V_{k+1} - V^\\pi||_\\infty \\leq \\gamma ||V_k - V^\\pi||_\\infty = \\frac{\\epsilon\\gamma}{1-\\gamma}$\n",
    "\n",
    "Hence, proved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Prove that the iterative policy evaluation algorithm converges geometrically, i.e\n",
    "\n",
    "$||V_{k+1} - V^\\pi||_\\infty \\leq \\gamma^k||V_1 - V^\\pi||_\\infty$\n",
    "\n",
    "<u> Answer: </u>\n",
    "\n",
    "We define the Bellman Evaluation Operator as $L^\\pi(v) = R^\\pi + \\gamma P^\\pi v $ where:\n",
    "\n",
    "$R^\\pi = \\sum\\limits_a \\pi(a|s)\\sum\\limits_{s'}P^a_{ss'}R^a_{ss'}$\n",
    "\n",
    "$P^\\pi = \\sum\\limits\\pi(a|s)P^a_{ss'}$\n",
    "\n",
    "Then, we know, \n",
    "\n",
    "$L^\\pi(V_k) = V_{k+1}$\n",
    "\n",
    "$L^\\pi(V^\\pi) = V^\\pi$ where $V^\\pi$ is a fixed point.\n",
    "\n",
    "Thus, we can write\n",
    "\n",
    "$||V_{k+1} - V^\\pi||_\\infty = ||L^\\pi(V_{k}) - L^\\pi(V^\\pi)|| \\leq \\gamma ||V_k - V^\\pi||_\\infty$\n",
    "\n",
    "using the fact that the Bellman Evaluation Operator is a $\\gamma-$ contraction on the space of all value functions. \n",
    "\n",
    "Thus, \n",
    "\n",
    "$||V_2 - V^\\pi||_\\infty \\leq \\gamma ||V_1 - V^\\pi||_\\infty$\n",
    "\n",
    "\n",
    "$||V_3 - V^\\pi||_\\infty \\leq \\gamma ||V_2 - V^\\pi||_\\infty \\leq \\gamma^2 ||V_1 - V^\\pi||_\\infty$\n",
    "\n",
    "$||V_4 - V^\\pi||_\\infty \\leq \\gamma ||V_3 - V^\\pi||_\\infty \\leq \\gamma^3 ||V_1 - V^\\pi||_\\infty$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$||V_{k+1} - V^\\pi||_\\infty \\leq \\gamma^k ||V_1 - V^\\pi||_\\infty$\n",
    "\n",
    "\n",
    "Hence, proved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Let $v$ denote a value function and consider the Bellman optimality operator given by:\n",
    "\n",
    "$\\mathcal{L}(v) =  \\max\\limits_{a \\in \\mathcal{A}}[\\mathcal{R}^a + \\gamma \\mathcal{P}^av]$\n",
    "\n",
    "Prove that the Bellman optimality operator $L$ satisfies the monotonicity property. For any two value functions $u$ and $v$ such that $u \\leq v$, we have $\\mathcal{L}(u) \\leq \\mathcal{L}(v)$.\n",
    "\n",
    "<u> Answer: </u>\n",
    "\n",
    "We know, $u \\leq v \\implies u(s) \\leq v(s) \\; \\forall s \\in \\mathcal{S}$\n",
    "\n",
    "Showing $\\mathcal{L}(u) \\leq \\mathcal{L}(v)$ is equivalent to showing $\\mathcal{L}(u)-\\mathcal{L}(v) \\leq 0$\n",
    "\n",
    "$\\mathcal{L}(u)-\\mathcal{L}(v) = \\max\\limits_{a \\in \\mathcal{A}}[\\mathcal{R}^a + \\gamma \\mathcal{P}^au] - \\max\\limits_{a \\in \\mathcal{A}}[\\mathcal{R}^a + \\gamma \\mathcal{P}^av]$\n",
    "\n",
    "Since, $u(s) \\leq v(s) \\; \\forall s \\in \\mathcal{S}$, we can say\n",
    "\n",
    "$\\gamma \\mathcal{P}^au \\leq \\gamma \\mathcal{P}^av$ since $\\gamma$ and $\\mathcal{P}^a$ are positive quantities.\n",
    "\n",
    "$\\mathcal{L}(u)-\\mathcal{L}(v) = \\max\\limits_{a \\in \\mathcal{A}}[\\mathcal{R}^a + \\gamma \\mathcal{P}^au] - \\max\\limits_{a \\in \\mathcal{A}}[\\mathcal{R}^a + \\gamma \\mathcal{P}^av] \\leq \\max\\limits_{a \\in \\mathcal{A}}[\\mathcal{R}^a + \\gamma \\mathcal{P}^av] - \\max\\limits_{a \\in \\mathcal{A}}[\\mathcal{R}^a + \\gamma \\mathcal{P}^av]$\n",
    "\n",
    "$\\implies \\mathcal{L}(u) - \\mathcal{L}(v) \\leq 0$\n",
    "\n",
    "$\\implies \\mathcal{L}(u) \\leq \\mathcal{L}(v)$\n",
    "\n",
    "Hence, proved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 2: On contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Let $P$ and $Q$ be contractions on the normed vector space $<\\mathcal{V} , ||.||>$. Prove that the compositions $P \\circ Q$ and $Q \\circ P$ are also compositions on the same vector space.\n",
    "\n",
    "<u> Answer: </u> By the definition of contractions, we know,\n",
    "\n",
    "$||P(x) - P(y)|| \\leq \\alpha||x-y|| \\; \\forall x,y \\in \\mathcal{V}$\n",
    "\n",
    "$||Q(x) - Q(y)|| \\leq \\beta||x-y|| \\; \\forall x,y \\in \\mathcal{V}$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$||P(Q(x)) - P(Q(y))|| \\leq \\alpha||Q(x) - Q(y)|| \\leq \\alpha\\beta||x-y|| = \\gamma||x-y||\\; \\forall x,y \\in \\mathcal{V}$\n",
    "\n",
    "\n",
    "$||Q(P(x)) - Q(P(y))|| \\leq \\beta||P(x) - P(y)|| \\leq \\alpha\\beta||x-y|| = \\gamma||x-y||\\; \\forall x,y \\in \\mathcal{V}$\n",
    "\n",
    "Thus, $P \\circ Q$ and $Q \\circ P$ are also compositions on the same vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What can be suitable contraction (or Lipschitz) coeffecients for the contractions $P \\circ Q$ and\n",
    "$Q\\circ P$ ?\n",
    "\n",
    "<u> Answer: </u> From the previous part, we can see that \n",
    "\n",
    "Lipschitz coeff($P \\circ Q$) = Lipschitz coeff($Q \\circ P$) = Lipschitz coeff(P) $\\times$ Lipschitz coeff(Q)\n",
    "\n",
    "This can be interpreted as if $P$ leads to a contraction by $c_P$, and $Q$ leads to a contraction by $c_Q$, then we get an overall contraction of $c_Pc_Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Define operator $\\mathcal{B} = \\mathcal{F} \\circ \\mathcal{L}$ where $\\mathcal{L}$ is the Bellman Optimality Operator and $\\mathcal{F}$ is any other suitable operator such as the function approximator to the Bellman backup $\\mathcal{L}$.  Under what conditions would the value iteration algorithm converge to a unique\n",
    "solution if operator $\\mathcal{B}$ is used in place of $\\mathcal{L}$?\n",
    "\n",
    "<u>Answer: </u>\n",
    "\n",
    "For convergence to a unique solution, we have seen by the Banach fixed Point Theorem, that contractions will lead to convergence. Thus, we need operator $\\mathcal{B}$ to be a contraction, i.e $\\mathcal{F} \\circ \\mathcal{L}$ needs to be a contraction, which we have proved in the first subpart. It requires $\\mathcal{F}$ to be a contraction as well under the $L_\\infty$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 3: Monte Carlo Methods\n",
    "\n",
    "Consider a Markov Process with 2 states $\\mathcal{S} = \\{S,A\\}$, where $p \\in (0,1)$. Assume rewards in state $S$ and $A$ are 1 and 0, and let $\\gamma = 1$.\n",
    "\n",
    "|     \t| $S$   \t| $A$ \t|\n",
    "|-----\t|-------\t|------\t|\n",
    "| $S$ \t| $1-p$ \t| $p$  \t|\n",
    "| $A$ \t| $0$     \t| $1$    \t|   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Provide a generic form for a typical trajectory starting from $S$.\n",
    "\n",
    "<u>Answer: </u> \n",
    "\n",
    "We have, $A$ is an absorbing state, and from $S$, we can stay at $S$, or move to A, and finish the trajectory.\n",
    "\n",
    "Thus, our typical trajectories would look like:\n",
    "\n",
    "$S,A$ with probability $p$\n",
    "\n",
    "$S,S,A$ with probability $(1-p)p$\n",
    "\n",
    "$S,S,S,A$ with probability $(1-p)^2p$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$\\underbrace{S, S \\ldots , S}_{n \\text{ times}},A$ with probability $(1-p)^{n-1}p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Estimate $V(S)$ using first-visit MC.\n",
    "\n",
    "<u> Answer: </u>\n",
    "\n",
    "<u><b>Finite case</b> </u>:\n",
    "\n",
    "Consider the trajectories:\n",
    "\n",
    "$S,A$ \n",
    "\n",
    "$S,S,A$ \n",
    "\n",
    "$S,S,S,A$ \n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$\\underbrace{S, S \\ldots , S}_{n \\text{ times}},A$ \n",
    "\n",
    "Then ,using first-visit MC, the total number of visits to state $S$ will be the number of trajectories, i.e $n$, and the reward for the $i^{th}$ trajectory will be i:\n",
    "\n",
    "$V(S) = \\frac{1}{n}\\left[1 + 2 + \\ldots n \\right] = \\frac{n(n+1)}{2n} = \\frac{n+1}{2}$\n",
    "\n",
    "\n",
    "<u><b>Infinite case</b> </u>:\n",
    "\n",
    "We would use the following Arithmetic-Geometric Progresion(AGP) result:\n",
    "\n",
    "$\\sum\\limits_{i=1}^\\infty ir^{i-1} = \\frac{1}{(1-r)^2}$ \n",
    "\n",
    "Proof: Let \n",
    "\n",
    "$S = \\sum\\limits_{i=1}^\\infty ir^{i-1} = 1 + 2r + 3r^2 + \\ldots$\n",
    "\n",
    "$rS = \\sum\\limits_{i=1}^\\infty ir^{i} = r + 2r^2 + 3r^3 + \\ldots$\n",
    "\n",
    "$\\implies (1-r)S = 1 + r + r^2 + r^3 + \\ldots = \\frac{1}{1-r}$\n",
    "\n",
    "$\\implies S = \\frac{1}{(1-r)^2}$\n",
    "\n",
    "Let $\\alpha$ be a normalising constant that allows us to convert the probabilities of trajectories into countable numbers.\n",
    "\n",
    "Then, we can say our trajectories would look like:\n",
    "\n",
    "$S,A$ with count $\\alpha p$\n",
    "\n",
    "$S,S,A$ with count $\\alpha(1-p)p$\n",
    "\n",
    "$S,S,S,A$ with count $\\alpha(1-p)^2p$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$\\underbrace{S, S \\ldots , S}_{n \\text{ times}},A$ with count $\\alpha(1-p)^{n-1}p$\n",
    "\n",
    "Then ,using first-visit MC, we can write:\n",
    "\n",
    "$V(S) = \\frac{1}{\\alpha p + \\alpha (1-p)p + \\alpha(1-p)^2p+ \\ldots}\\left[\\alpha p \\times 1 + \\alpha (1-p)p\\times 2 + \\alpha(1-p)^2p \\times 3 \\ldots \\right]$\n",
    "\n",
    "where the first fraction denotes the number of first visits to state $S$, and the subsequent sum gives us the reward obtained in every trajectory on the first visit to $S$.\n",
    "\n",
    "$\\implies V(S) = \\frac{1}{\\alpha p \\sum\\limits_{i=0}^\\infty (1-p)^i}\\left[\\alpha p\\sum\\limits_{i=1}^\\infty i(1-p)^{i-1}\\right] = \\frac{1}{\\sum\\limits_{i=0}^\\infty (1-p)^i}\\left[\\sum\\limits_{i=1}^\\infty i(1-p)^{i-1}\\right]$\n",
    "\n",
    "$\\implies V(S) = \\frac{1}{\\frac{1}{1 - (1-p)}}\\left[\\frac{1}{(1 - (1-p))^2}\\right]$\n",
    "\n",
    "$\\implies V(S) = \\frac{1}{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Estimate $V(S)$ using every-visit MC.\n",
    "\n",
    "<u> Answer: </u>\n",
    "\n",
    "<u><b>Finite case</b> </u>:\n",
    "\n",
    "Consider the trajectories:\n",
    "\n",
    "$S,A$ \n",
    "\n",
    "$S,S,A$ \n",
    "\n",
    "$S,S,S,A$ \n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$\\underbrace{S, S \\ldots , S}_{n \\text{ times}},A$ \n",
    "\n",
    "Then ,using every-visit MC, we can write V(s) as:\n",
    "\n",
    "$V(S) = \\frac{1}{1 + 2 + \\ldots + n}\\left[(1) + (2 + 1) + (3+2 + 1) + \\ldots + (n + (n-1) + \\ldots +2 + 1) \\right]$\n",
    "\n",
    "$\\implies V(s) = \\frac{\\sum\\limits_{i=1}^n \\frac{i(i+1)}{2}}{\\sum\\limits_{i=1}^n i} $\n",
    "\n",
    "$\\implies V(s) = \\frac{1}{2}\\left[\\frac{\\frac{n(n+1)(2n+1)}{6}}{\\frac{n(n+1)}{2}}\\right] + \\frac{1}{2} = \\frac{2n+1}{6} + \\frac{1}{2} = \\frac{n+2}{3}$\n",
    "\n",
    "\n",
    "<u><b>Infinite case</b> </u>:\n",
    "\n",
    "\n",
    "We would use the following Arithmetic-Geometric Progresion(AGP) results:\n",
    "\n",
    "$\\sum\\limits_{i=1}^\\infty ir^{i-1} = \\frac{1}{(1-r)^2}$ \n",
    "\n",
    "$\\sum\\limits_{i=1}^\\infty \\frac{i(i+1)}{2}r^{i-1} = \\frac{1}{(1-r)^3}$ \n",
    "\n",
    "Proof: Let \n",
    "\n",
    "$S = \\sum\\limits_{i=1}^\\infty  \\frac{i(i+1)}{2}r^{i-1} = 1 + 3r + 6r^2 + \\ldots$\n",
    "\n",
    "$rS = \\sum\\limits_{i=1}^\\infty  \\frac{i(i+1)}{2}r^{i-1} = r + 3r^2 + 6r^3 + \\ldots$\n",
    "\n",
    "$\\implies (1-r)S = 1 + 2r + 3r^2 + 4r^3 + \\ldots = \\frac{1}{(1-r)^2}$\n",
    "\n",
    "$\\implies S = \\frac{1}{(1-r)^3}$\n",
    "\n",
    "Let $\\alpha$ be a normalising constant that allows us to convert the probabilities of trajectories into countable numbers.\n",
    "\n",
    "Then, we can say our trajectories would look like:\n",
    "\n",
    "$S,A$ with count $\\alpha p$\n",
    "\n",
    "$S,S,A$ with count $\\alpha(1-p)p$\n",
    "\n",
    "$S,S,S,A$ with count $\\alpha(1-p)^2p$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$\\underbrace{S, S \\ldots , S}_{n \\text{ times}},A$ with count $\\alpha(1-p)^{n-1}p$\n",
    "\n",
    "Then ,using every-visit MC, we can write:\n",
    "\n",
    "$V(S) = \\frac{1}{\\alpha p + \\alpha (1-p)p \\times 2 + \\alpha(1-p)^2p \\times 3+ \\ldots}\\left[\\alpha p \\times 1 + \\alpha (1-p)p\\times (2 +1) + \\alpha(1-p)^2p \\times (3 + 2 + 1)\\ldots \\right]$\n",
    "\n",
    "where the first fraction denotes the total number of visits to state $S$, and the subsequent sum gives us the reward obtained in every trajectory on a particular visit to $S$.\n",
    "\n",
    "$\\implies V(S) = \\frac{1}{\\alpha p \\sum\\limits_{i=1}^\\infty i(1-p)^{i-1}}\\left[\\alpha p\\sum\\limits_{i=1}^\\infty [i + (i-1) + (i-2) \\ldots +2 + 1](1-p)^{i-1}\\right] = \\frac{1}{\\sum\\limits_{i=1}^\\infty i(1-p)^{i-1}}\\left[\\sum\\limits_{i=1}^\\infty \\frac{i(i+1)}{2}(1-p)^{i-1}\\right]$\n",
    "\n",
    "$\\implies V(S) = \\frac{1}{\\frac{1}{(1 - (1-p))^2}}\\left[\\frac{1}{(1 - (1-p))^3}\\right]$\n",
    "\n",
    "$\\implies V(S) = \\frac{1}{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) What is true value of $V(S)$?\n",
    "\n",
    "<u> Answer: </u>\n",
    "\n",
    "We have $V(A) = 0$\n",
    "\n",
    "We know, \n",
    "\n",
    "$V(s_t = s) = r_{t+1} + \\gamma[\\sum\\limits_{s'}\\mathcal{P}_{ss'}V(s')]$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$V(s) = 1 + pV(A) + (1-p)V(s)$\n",
    "\n",
    "$\\implies V(s) = \\frac{1}{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Explain if the every-visit MC is biased.\n",
    "\n",
    "<u> Answer:</u> Bias is defined as the error in the true model and the estimated value , i.e if the true model is given by $\\theta$, and the predicted model is given by $\\hat{\\theta}$. Here, we see  in the finite case that there is some error, and thus some bias. However, as we increase the number of trajectories to infinity, both the true values, as well as the predicted value using Every Vist Monte Carlo Method is equal to $\\frac{1}{p}$, thus, leading to no bias. This is because, using the law of large numbers, as the number of trajectories tend to infinity, which we have done in the Every Visit MC, the value should converge to the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Comment on the convergence properties of the first visit MC and every\n",
    "visit MC algorithms.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "First-Visit Monte Carlo relies on the law of Large number to converge. Every-Visit Monte Carlo also converges, but the convergence is more difficult and sophesticated as compared to the convergence of First-Visit Monte Carlo due to the independence of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 4: Temporal Difference Methods\n",
    "\n",
    "Consider one step TD error given by:\n",
    "\n",
    "$\\delta_t = r_{t+1} + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Find $\\mathbb{E}_\\pi[\\delta_t | s_t = s]$ assuming $\\delta_t$ uses the true state value function $V^\\pi$.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "Using the definition of $\\delta_t$, we get:\n",
    "\n",
    "$\\mathbb{E}_\\pi[\\delta_t | s_t = s] = \\mathbb{E}_\\pi[(r_{t+1} + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t)) | s_t = s]$\n",
    "\n",
    "Using the linearity of expectation, this can be written as:\n",
    "\n",
    "$\\mathbb{E}_\\pi[\\delta_t | s_t = s] = \\mathbb{E}_\\pi[r_{t+1} + \\gamma V^\\pi(s_{t+1})|s_t = s] - \\mathbb{E}_\\pi[V^\\pi(s_t) | s_t = s]$\n",
    "\n",
    "$\\implies \\mathbb{E}_\\pi[\\delta_t | s_t = s] = V^\\pi(s) - \\mathbb{E}_\\pi[V^\\pi(s_t) | s_t = s]$\n",
    "\n",
    "using the definition of $V^\\pi(S)$\n",
    "\n",
    "$\\implies \\mathbb{E}_\\pi[\\delta_t | s_t = s] = V^\\pi(s) - \\mathbb{E}_\\pi[V^\\pi(s_t) | s_t = s] = V^\\pi(s) - V^\\pi(s) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Find $\\mathbb{E}_\\pi[\\delta_t | s_t = s , a_t = a]$ for an arbitrary action $a$, assuming $\\delta_t$ uses the true state value function $V^\\pi$.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "Using the definition of $\\delta_t$:\n",
    "\n",
    "$\\mathbb{E}_\\pi[\\delta_t | s_t = s , a_t = a] = \\mathbb{E}_\\pi[r_{t+1} + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t) | s_t = s , a_t = a] $\n",
    "\n",
    "Using the Linearity of Expectation, \n",
    "\n",
    "$\\mathbb{E}_\\pi[\\delta_t | s_t = s , a_t = a] = \\mathbb{E}_\\pi[r_{t+1} + \\gamma V^\\pi(s_{t+1})| s_t = s , a_t = a] - \\mathbb{E}_\\pi[V^\\pi(s_t) | s_t = s , a_t = a] $\n",
    "\n",
    "Note that the second term $\\mathbb{E}_\\pi[V^\\pi(s_t) | s_t = s , a_t = a] = \\mathbb{E}_\\pi[V^\\pi(s) | a_t = a]$\n",
    "\n",
    "Since the Value function is independent of the action taken, it is simply equal to $V^\\pi(s)$\n",
    "\n",
    "The first term involves $\\mathbb{E}_\\pi[V^\\pi(s_{t+1})| s_t = s , a_t = a]$. Note that this term is equal to $Q^\\pi(s,a)$. This is because, $Q^\\pi(s,a)$ is the expected value of a state $s$, after taking an action $a$. Here, given that our current state is $s$, and the action we take is $a$, we want the expected value of our next possible states, which is precisely $Q^\\pi(s,a)$.\n",
    "\n",
    "$\\mathbb{E}_\\pi[r_{t+1} + \\gamma V^\\pi(s_{t+1})| s_t = s , a_t = a] = \\mathbb{E}_\\pi[r_{t+1} + \\mathbb{E}_\\pi[\\gamma \\sum\\limits_{k=0}^\\infty \\gamma^k r_{(t+1) + k+1} | s_{t+1}]| s_t = s , a_t = a]$\n",
    "\n",
    "\n",
    "$\\mathbb{E}_\\pi[r_{t+1} + \\gamma V^\\pi(s_{t+1})| s_t = s , a_t = a] = \\mathbb{E}_\\pi[r_{t+1} + \\mathbb{E}_\\pi[\\sum\\limits_{k=0}^\\infty \\gamma^{k+1} r_{(t+1) + k+1} ]| s_t = s , a_t = a , s_{t+1}]$\n",
    "\n",
    "\n",
    "$\\mathbb{E}_\\pi[r_{t+1} + \\gamma V^\\pi(s_{t+1})| s_t = s , a_t = a] = \\mathbb{E}_\\pi[\\mathbb{E}_\\pi[\\sum\\limits_{k=0}^\\infty \\gamma^{k} r_{(t+ k+1)} ]| s_t = s , a_t = a]$\n",
    "\n",
    "using the Markov Property. Also, note, we can ignore the external expectation since its argument itself is an expectation.\n",
    "\n",
    "$\\mathbb{E}_\\pi[r_{t+1} + \\gamma V^\\pi(s_{t+1})| s_t = s , a_t = a] = \\mathbb{E}_\\pi[\\sum\\limits_{k=0}^\\infty \\gamma^{k} r_{(t+ k+1)} | s_t = s , a_t = a] = Q^\\pi(s,a)$\n",
    "\n",
    "using the definition of $Q^\\pi(s,a)$\n",
    "\n",
    "Thus, we get $\\mathbb{E}_\\pi[\\delta_t | s_t = s , a_t = a] = Q^\\pi(s,a) - V^\\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Consider the $TD(\\lambda)$ algorithm:\n",
    "\n",
    "$G_t^\\lambda = (1-\\lambda) \\sum\\limits_{n=1}^\\infty \\lambda^{n-1}G_t^{(n)}$\n",
    "\n",
    "where $G_t^{(n)}$ is the n-step return defined as\n",
    "\n",
    "$G_t^{n} = r_{t+1} + \\gamma r_{t+2} + \\ldots + \\gamma^{n-1}r_{t+n} + \\gamma^nV(s_{t+n})$\n",
    "\n",
    "Let $\\eta(\\lambda)$ denote the time by which the weighting sequence would have fallen to half of its initial value. Compute the value of $\\lambda$ for which the weights would drop to half after 3 step returns.\n",
    "\n",
    "<u> Answer </u>\n",
    "\n",
    "We know, the weight sequence is given by $(1-\\lambda) , \\lambda(1-\\lambda) , \\lambda^2(1-\\lambda) \\ldots$\n",
    "\n",
    "Let the weights drop to half at the $\\eta^{th}$ term, then:\n",
    "\n",
    "$\\frac{1}{2} (1-\\lambda) = \\lambda^\\eta(1-\\lambda)$\n",
    "\n",
    "$\\implies \\eta(\\lambda) = \\log_{\\lambda}\\frac{1}{2} = \\frac{1}{\\log_{\\frac{1}{2}}\\lambda}$\n",
    "\n",
    "Substituting $ \\eta = 3$, we get\n",
    "\n",
    "$\\lambda = \\left(\\frac{1}{2}\\right)^\\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 5: On learning rates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point out if the following values of $\\alpha_t$ would lead to convergence:\n",
    "\n",
    "1. $\\alpha_t = \\frac{1}{t}$\n",
    "\n",
    "<u>Answer:</u> We use the integral test, which says that a particular sum converge if and only of the intergal converges. We require that $\\sum\\limits_{i=1}^\\infty \\alpha_t$ diverges, i.e $\\int_1^\\infty \\alpha_t \\,dt$ diverges,  and $\\sum\\limits_{i=1}^\\infty \\alpha_t^2$ converges, i.e $\\int_1^\\infty \\alpha_t^2 \\,dt$.\n",
    "\n",
    "Consider $\\alpha_t = \\frac{1}{t}$. Then,\n",
    "\n",
    "$\\int_1^\\infty \\alpha_t \\, dt = \\int_1^\\infty \\frac{1}{t} \\, dt = \\ln t |^{\\infty}_1 = \\lim\\limits_{t\\rightarrow\\infty}\\ln t - 0 = \\infty$\n",
    "\n",
    "$\\int_1^\\infty \\alpha_t^2 \\, dt = \\int_1^\\infty \\frac{1}{t^2} \\, dt = -\\frac{1}{t} |^{\\infty}_1 = -\\left[\\lim\\limits_{t\\rightarrow\\infty}\\frac{1}{t} - 1\\right] = 1$\n",
    "\n",
    "Thus, $\\alpha_t  = \\frac{1}{t} $ will result in convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $\\alpha_t = \\frac{1}{t^2}$\n",
    "\n",
    "<u>Answer:</u>\n",
    "\n",
    "$\\int_1^\\infty \\alpha_t \\, dt= \\int_1^\\infty \\frac{1}{t^2} \\, dt = -\\frac{1}{t} |^{\\infty}_1 = -\\left[\\lim\\limits_{t\\rightarrow\\infty}\\frac{1}{t} - 1\\right] = 1$\n",
    "\n",
    "$\\int_1^\\infty \\alpha_t^2 \\, dt = \\int_1^\\infty \\frac{1}{t^4} \\, dt = \\frac{-1}{3t^3} |^{\\infty}_1 = \\lim\\limits_{t\\rightarrow\\infty}\\frac{-1}{3t^3} + \\frac{1}{3} = \\frac{1}{3}$\n",
    "\n",
    "Since, $\\int_1^\\infty \\alpha_t \\, dt$ does not diverge, $\\alpha_t = \\frac{1}{t^2}$ will not result in covergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. $\\alpha_t = \\frac{1}{t^\\frac{2}{3}}$\n",
    "\n",
    "<u>Answer:</u>\n",
    "\n",
    "$\\int_1^\\infty \\alpha_t \\, dt= \\int_1^\\infty \\frac{1}{t^\\frac{2}{3}} \\, dt = 3t^{\\frac{1}{3}} |^{\\infty}_1 = \\lim\\limits_{t\\rightarrow\\infty}3t^{\\frac{1}{3}} - 3\\ = \\infty$\n",
    "\n",
    "$\\int_1^\\infty \\alpha_t^2 \\, dt = \\int_1^\\infty \\frac{1}{t^\\frac{4}{3}} \\, dt = \\frac{-3}{t^\\frac{1}{3}} |^{\\infty}_1 = \\lim\\limits_{t\\rightarrow\\infty}\\frac{-3}{t^\\frac{1}{3}} +3 = 3$\n",
    "\n",
    "Thus, $\\alpha_t = \\frac{1}{t^\\frac{2}{3}}$ will result in covergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. $\\alpha_t = \\frac{1}{t^\\frac{1}{2}}$\n",
    "\n",
    "<u>Answer:</u>\n",
    "\n",
    "$\\int_1^\\infty \\alpha_t \\, dt= \\int_1^\\infty \\frac{1}{t^\\frac{1}{2}} \\, dt = 2t^{\\frac{1}{2}} |^{\\infty}_1 = \\lim\\limits_{t\\rightarrow\\infty}2t^{\\frac{1}{1}} - 2\\ = \\infty$\n",
    "\n",
    "$\\int_1^\\infty \\alpha_t^2 \\, dt = \\int_1^\\infty \\frac{1}{t} \\, dt = \\ln t |^{\\infty}_1 = \\lim\\limits_{t\\rightarrow\\infty}\\ln t - 0 = \\infty$\n",
    "\n",
    "Since, $\\int_1^\\infty \\alpha_t^2 \\, dt$ does not converge, $\\alpha_t = \\frac{1}{t^\\frac{1}{2}}$ will not result in covergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalise this result for $\\alpha_t = \\frac{1}{t^p}$ for any positive real number $p \\in \\mathbb{R}^+$\n",
    "\n",
    "<u>Answer:</u> \n",
    "\n",
    "$\\int_1^\\infty \\alpha_t \\, dt= \\int_1^\\infty \\frac{1}{t^p} \\, dt = \\frac{t^{-p+1}}{-p+1} |^{\\infty}_1 = \\lim\\limits_{t\\rightarrow\\infty}\\frac{t^{1-p}}{1-p} - \\frac{1}{1-p}$\n",
    "\n",
    "Since we want $\\int_1^\\infty \\alpha_t \\, dt$ to diverge, we have $1-p > 0 \\implies p < 1$. We have already seen $\\int_1^\\infty \\frac{1}{t} \\, dt$ is diverging, thus, we can say for $\\int_1^\\infty \\alpha_t \\, dt$ diverges for $p\\leq1$\n",
    "\n",
    "$\\int_1^\\infty \\alpha_t^2 \\, dt= \\int_1^\\infty \\frac{1}{t^{2p}} \\, dt = \\frac{t^{1-2p}}{1-2p} |^{\\infty}_1 = \\lim\\limits_{t\\rightarrow\\infty}\\frac{t^{1-2p}}{1-2p} - \\frac{1}{1-2p}$\n",
    "\n",
    "Since we want $\\int_1^\\infty \\alpha_t^2 \\, dt$ to converge, we have $1-2p < 0 \\implies p > \\frac{1}{2}$. Note that for $p = \\frac{1}{2}$, the integral fails to converge.\n",
    "\n",
    "Thus, we get for $\\alpha_t$ to result in convergence, we have $p \\in \\left(\\frac{1}{2} , 1\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 6: Programming Policy and Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env , gamma , error_tol , policy , if_policy = False):\n",
    "    \n",
    "    nA = len(env.env.P[0])\n",
    "    nS = len(env.env.P)\n",
    "\n",
    "    # initialising values\n",
    "    V = np.zeros((nS ,))\n",
    "\n",
    "    if not if_policy:\n",
    "        \n",
    "        flag = True\n",
    "\n",
    "        while flag:\n",
    "\n",
    "            new_V = np.zeros((nS ,))\n",
    "\n",
    "            for s in range(nS):\n",
    "\n",
    "                per_action = np.zeros((nA ,))\n",
    "\n",
    "                for a in range(nA):\n",
    "\n",
    "                    for nS_dash in range(len(env.env.P[s][a])):\n",
    "\n",
    "                        p = env.env.P[s][a][nS_dash][0]\n",
    "                        s_dash= env.env.P[s][a][nS_dash][1]\n",
    "                        r = env.env.P[s][a][nS_dash][2]\n",
    "                        per_action[a] += (p * (r + gamma * V[s_dash]))\n",
    "\n",
    "                new_V[s] = np.max(per_action)\n",
    "\n",
    "            if np.max(np.abs(V - new_V)) < error_tol:\n",
    "                V = new_V\n",
    "                flag = False\n",
    "                \n",
    "            else:\n",
    "                V = new_V\n",
    "\n",
    "        return V\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # assuming DETERMINISTIC policy\n",
    "\n",
    "        flag = True\n",
    "\n",
    "        while flag:\n",
    "\n",
    "            new_V = np.zeros((nS ,))\n",
    "\n",
    "            for s in range(nS):\n",
    "\n",
    "                sum = 0\n",
    "\n",
    "                for nS_dash in range(len(env.env.P[s][policy[s]])):\n",
    "\n",
    "                    p = env.env.P[s][policy[s]][nS_dash][0]\n",
    "                    s_dash= env.env.P[s][policy[s]][nS_dash][1]\n",
    "                    r = env.env.P[s][policy[s]][nS_dash][2]\n",
    "                    sum += (p * (r + gamma * V[s_dash]))\n",
    "\n",
    "                new_V[s] = sum\n",
    "\n",
    "            if np.max(np.abs(V - new_V)) < error_tol:\n",
    "                V = new_V\n",
    "                flag = False\n",
    "                \n",
    "            else:\n",
    "                V = new_V\n",
    "\n",
    "        return V\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env , gamma , error_tol , n_iters = 500):\n",
    "\n",
    "    nA = len(env.env.P[0])\n",
    "    nS = len(env.env.P)\n",
    "\n",
    "    # initialising values\n",
    "    policies = np.zeros((nS ,))\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        V = value_iteration(env , gamma , error_tol , policies , True)\n",
    "\n",
    "        new_policy = np.zeros((nS ,))\n",
    "\n",
    "        for s in range(nS):\n",
    "\n",
    "            per_action = np.zeros((nA ,))\n",
    "\n",
    "            for a in range(nA):\n",
    "\n",
    "                for nS_dash in range(len(env.env.P[s][a])):\n",
    "\n",
    "                    p = env.env.P[s][a][nS_dash][0]\n",
    "                    s_dash= env.env.P[s][a][nS_dash][1]\n",
    "                    r = env.env.P[s][a][nS_dash][2]\n",
    "                    per_action[a] += (p * (r + gamma * V[s_dash]))\n",
    "\n",
    "            new_policy[s] = np.argmax(per_action)\n",
    "\n",
    "            policies = new_policy\n",
    "            # print(new_policy.reshape((4,4)))\n",
    "\n",
    "    return policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "# \"SFFF\",\n",
    "# \"FHFH\",\n",
    "# \"FFFH\",\n",
    "# \"HFFG\"\n",
    "\n",
    "# 0->left\n",
    "# 1-> down\n",
    "# 2-> right\n",
    "# 3-> up\n",
    "\n",
    "# is slippery -> leads to environmental uncertainity in actions, \n",
    "# thus taking action 'a' results in all other actions other than 'a'\n",
    "\n",
    "# env.env.P understanding -> for state 0, for each action, the other 3 actions are possible with prob 1/3\n",
    "# bools refer to whether terminated or not\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82329454 0.82321581 0.8231599  0.8231309 ]\n",
      " [0.82331164 0.         0.52924383 0.        ]\n",
      " [0.82334458 0.82339098 0.7645838  0.        ]\n",
      " [0.         0.88225465 0.94112547 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# VALUE ITERATION\n",
    "\n",
    "print(value_iteration(env, 1 , 0.00001 , []).reshape((4,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 3. 3. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# POLICY ITERATION\n",
    "\n",
    "print(policy_iteration(env , 1 , 0.00001 , 500).reshape((4,4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0b3843ef66ef72e0d095359ab408deb80cd37476039c722f337af7e4c5c1d89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
