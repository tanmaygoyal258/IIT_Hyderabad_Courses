{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS5803 NLP\n",
    "### Assignment 2\n",
    "#### Tanmay Garg, Tanmay Goyal, Tanay Yadav\n",
    "#### Roll no: CS20BTECH11063, AI20BTECH11021, AI20BTECH11026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Link to dataset: https://www.kaggle.com/datasets/moxxis/harry-potter-lstm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sumit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sumit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing necessary packages\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from math import log, exp  \n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1. Preprocess and tokenize the dataset using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the original data\n",
    "with open('Harry_Potter_all_char_separated.txt', 'r', encoding='utf-8') as file:\n",
    "    harry_potter_data = file.read()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    '''\n",
    "    Function to preprocess the text by removing punctuations and converting to lower case\n",
    "    '''\n",
    "    # [^\\w\\s] -> ^ means except , \\w refers to any alphanumeric character and \\s refers to whitespace    \n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    '''\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    # stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# preprocessing the text\n",
    "harry_potter_text = preprocess_text(harry_potter_data)\n",
    "# using the first 10000 words\n",
    "harry_potter_tokens = tokenize(harry_potter_text)[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2. Fit two bigram language models on the text: MLE and kneserNey Discounting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit two bigram language models on the text: MLE and Kneser-Ney discounting using the nltk library\n",
    "\n",
    "def MLE_bigram(n , n_gram , vocab):\n",
    "    '''\n",
    "    Function to fit a bigram language model using MLE\n",
    "    '''\n",
    "    model = nltk.lm.MLE(n)\n",
    "    model.fit([n_gram], vocabulary_text = vocab)\n",
    "    return model\n",
    "\n",
    "def KN_bigram(n , n_gram , vocab):\n",
    "    '''\n",
    "    Function to fit a bigram language model using Kneser-Ney discounting\n",
    "    '''\n",
    "    model = nltk.lm.KneserNeyInterpolated(n)\n",
    "    model.fit([n_gram], vocabulary_text = vocab)\n",
    "    return model\n",
    "\n",
    "harry_potter_bigrams = nltk.ngrams(harry_potter_tokens, 2)\n",
    "\n",
    "# converting the bigrams to a list\n",
    "bigrams = []\n",
    "for bigram in harry_potter_bigrams:\n",
    "    bigrams.append(bigram)\n",
    "\n",
    "bigram_mle = MLE_bigram(2 , bigrams , harry_potter_tokens)\n",
    "bigram_kn = KN_bigram(2 , bigrams , harry_potter_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3. Use the beginning words 1. \"Harry Potter\" and 2. \"Dumbledore\" to generate text using both the language models. Keep maximum text length as 20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== MLE Model ====\n",
      "harry potter are are less like us all right of them angrily it he does tend to be nice if hed just\n",
      "dumbledore and bacon as dudley was a man had swapped at his favorite program had expected mrs dursleys bought dudley he\n",
      "==== KneserNey Model ====\n",
      "harry potter are are less like us all right place you cant ive finished dialing his hair and piers and had four\n",
      "dumbledore and bacon as dudley was a mad old things gray tuesday our heads down old clothes of arms and james\n"
     ]
    }
   ],
   "source": [
    "def generate_prediction(model , num_words = None , text_seed = None , random_seed = None):\n",
    "    '''\n",
    "    Function to generate the predictions given text_seed and num_words.\n",
    "    It joins them together in a sentence and returns the sentence\n",
    "    '''\n",
    "\n",
    "    # preprocessing the text_seed\n",
    "    text_seed = preprocess_text(text_seed)\n",
    "    text_seed = tokenize(text_seed)\n",
    "    \n",
    "    # generating the prediction\n",
    "    pred = model.generate(num_words = num_words , text_seed = text_seed , random_seed = random_seed)\n",
    "    \n",
    "    sentence = text_seed + [pred[i] for i in range(num_words)]\n",
    "    predicted_sentence = sentence[0]\n",
    "    for word in sentence[1:]:\n",
    "        predicted_sentence += ' ' + word\n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "print(\"==== MLE Model ====\")\n",
    "print(generate_prediction(bigram_mle , num_words = 20 , text_seed = \"Harry Potter\" , random_seed = 123))\n",
    "print(generate_prediction(bigram_mle , num_words = 20 , text_seed = \"Dumbledore\" , random_seed = 123)) \n",
    "\n",
    "print(\"==== KneserNey Model ====\")\n",
    "print(generate_prediction(bigram_kn , num_words = 20 , text_seed = \"Harry Potter\" , random_seed = 123))\n",
    "print(generate_prediction(bigram_kn , num_words = 20 , text_seed = \"Dumbledore\" , random_seed = 123)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search is a tree-based search strategy similar to BFS. In BF, we expand every child node, however, in Beam Search, we only expand the top k most probable children. The generated text is the text with the highest probabiltity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4. To implement beam search, implement a function to find the top k most probable words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_top_probable(model , k , text_seed):\n",
    "    '''\n",
    "    returns the top-k most probable words based on the model given\n",
    "    ''' \n",
    "    \n",
    "    # preprocessing the text_seed\n",
    "    text_seed = [w.lower() for w in text_seed]\n",
    "    # we store the non-zero probabilities\n",
    "    non_zero_prob = {}\n",
    "\n",
    "    for w in model.vocab:\n",
    "        if model.score(w , text_seed) > 0:\n",
    "            non_zero_prob[w] = model.score(w , text_seed)\n",
    "\n",
    "    # sorting the non_zero_prob based on values\n",
    "    sorted_probabilities = dict(sorted(non_zero_prob.items() , key = lambda item:item[1], reverse=True))\n",
    "    if len(non_zero_prob) > k:\n",
    "        return list(sorted_probabilities.keys())[:k]\n",
    "    \n",
    "    else:\n",
    "        top = list(sorted_probabilities.keys())\n",
    "\n",
    "        return top + [None]*(k - len(top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pink', 'mustache', 'tawny', 'order', 'doughnut']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_top_probable(bigram_mle, 5, [\"large\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5. Implement the Beam search using the previously trained MLE model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearch:\n",
    "    \n",
    "    generated_sentences = {}\n",
    "    queue = []\n",
    "    \n",
    "    def __init__(self , k , current_sentence , probability , depth , max_depth):\n",
    "        self.k = k\n",
    "        self.current_sentence = current_sentence\n",
    "        self.top_probs = k_top_probable(bigram_mle, k , [self.current_sentence[-1]])\n",
    "        self.children = []\n",
    "        self.probability = probability\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        BeamSearch.queue = [] if self.depth == 0 else BeamSearch.queue\n",
    "        BeamSearch.generated_sentences = {} if self.depth == 0 else BeamSearch.generated_sentences\n",
    "        BeamSearch.queue.append(self)\n",
    "\n",
    "    def generate_tree(node):\n",
    "        '''\n",
    "        Function to generate the tree\n",
    "        '''\n",
    "\n",
    "        BeamSearch.queue = BeamSearch.queue[1:]\n",
    "\n",
    "        if node.depth == node.max_depth:\n",
    "            BeamSearch.generated_sentences[tuple(node.current_sentence)] = node.probability\n",
    "            if len(BeamSearch.queue) != 0:\n",
    "                BeamSearch.generate_tree(BeamSearch.queue[0])\n",
    "                return\n",
    "            else:\n",
    "                print(\"Done constructing the trees\")\n",
    "                return\n",
    "\n",
    "        for word in node.top_probs:\n",
    "            if word is not None:\n",
    "                new_sentence = node.current_sentence + [word]\n",
    "                node.children.append(BeamSearch(node.k , new_sentence , node.probability * bigram_mle.score(word , [node.current_sentence[-1]]) , node.depth + 1 , node.max_depth))\n",
    "        \n",
    "        if len(BeamSearch.queue) != 0:\n",
    "            BeamSearch.generate_tree(BeamSearch.queue[0])\n",
    "        return\n",
    "\n",
    "    def best_sentences(self , number_of_sentences):\n",
    "        '''\n",
    "        Function to return the best sentence\n",
    "        '''\n",
    "        final_sorted = sorted(BeamSearch.generated_sentences.items() , key = lambda item:item[1], reverse=True)\n",
    "        print(\"Best sentences\")\n",
    "        for i in range(number_of_sentences):\n",
    "            s = final_sorted[i][0]\n",
    "            complete_sentence = s[0]\n",
    "        \n",
    "            for word in s[1:]:\n",
    "                complete_sentence += ' ' + word\n",
    "            print(i+1 , \". \" , complete_sentence)\n",
    "            print(\"Probability: \", final_sorted[i][1])\n",
    "            print(\"\\n\")\n",
    "\n",
    "        return final_sorted[:number_of_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q6. Implement Beam search for k=2 and depth = 10. Find the 5 generated texts with the highest probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done constructing the trees\n",
      "Best sentences\n",
      "1 .  harry had a large pink beach ball wearing an emerald green\n",
      "Probability:  4.1452323609999957e-07\n",
      "\n",
      "\n",
      "2 .  harry was a large pink beach ball wearing an emerald green\n",
      "Probability:  2.6321621231072855e-07\n",
      "\n",
      "\n",
      "3 .  harry had a large pink beach ball wearing an emerald one\n",
      "Probability:  2.0726161804999978e-07\n",
      "\n",
      "\n",
      "4 .  harry was a large pink beach ball wearing an emerald one\n",
      "Probability:  1.3160810615536428e-07\n",
      "\n",
      "\n",
      "5 .  harry had been a large pink beach ball wearing an emerald\n",
      "Probability:  9.420982638636354e-08\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = BeamSearch(2 , ['harry'] , 1 , 0 , 10)\n",
    "tree.generate_tree()\n",
    "_ = tree.best_sentences(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done constructing the trees\n",
      "Best sentences\n",
      "1 .  dumbledore and a large pink beach ball wearing an emerald green\n",
      "Probability:  1.4618533099840802e-07\n",
      "\n",
      "\n",
      "2 .  dumbledore and a large pink beach ball wearing an emerald one\n",
      "Probability:  7.309266549920401e-08\n",
      "\n",
      "\n",
      "3 .  dumbledore you know who was a large pink beach ball wearing\n",
      "Probability:  4.470384040603406e-08\n",
      "\n",
      "\n",
      "4 .  dumbledore and dudley had a large pink beach ball wearing a\n",
      "Probability:  4.111462434330226e-08\n",
      "\n",
      "\n",
      "5 .  dumbledore you cant blame her sister marge who lived chapter the\n",
      "Probability:  2.422621470240518e-08\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = BeamSearch(2 , ['dumbledore'] , 1 , 0 , 10)\n",
    "tree.generate_tree()\n",
    "_ = tree.best_sentences(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "\n",
    "    '''\n",
    "    Nodes for any tree to construct.\n",
    "    Consists of the parent, state and children\n",
    "    Children are stored in a list\n",
    "\n",
    "    Methods:\n",
    "    > addChild: To add a child to the node\n",
    "    > isLeaf: To check if the node is a leaf\n",
    "    > isRoot: To check if the node is a root\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, parent, state):\n",
    "        self.parent = parent\n",
    "        self.state = state\n",
    "        self.children = []\n",
    "\n",
    "    def addChild(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def isLeaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def isRoot(self):\n",
    "        return self.parent is None\n",
    "\n",
    "\n",
    "class BeamSearchV2(Node):\n",
    "\n",
    "    '''\n",
    "    Beam Search algorithm to generate the best N sentences based on the given model and root state\n",
    "\n",
    "    Constructor arguments:\n",
    "    > model: (any) The language model to use\n",
    "    > kbestfn: (any) The function to get the k best probable words\n",
    "    > root: (any) The root state to start the search\n",
    "    > beam_size: (int) The beam size\n",
    "    > max_depth: (int) The maximum depth to search\n",
    "    > return_sentences: (default: False) Whether to return the sentences or not\n",
    "    > num_best_sentences: (default: None) The number of best sentences to return\n",
    "\n",
    "    Methods:\n",
    "    > buildTree: To build the tree based on the given root and max_depth\n",
    "    > getAllPaths: To get all the paths from the root to the leaf nodes\n",
    "    > getNBestPaths: To get the best N paths based on the scores\n",
    "    > printBestNPaths: To print the best N paths\n",
    "    > printSentence: To print the sentence from the path\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model, kbestfn, root, beam_size, max_depth, return_sentences=False, num_best_sentences=None):\n",
    "        \n",
    "        self.root = Node(parent=None, state=root)\n",
    "        self.model = model\n",
    "        self.beam_size = beam_size\n",
    "        self.max_depth = max_depth\n",
    "        self.getKBest = kbestfn\n",
    "        self.pathsWithScore = {}\n",
    "        self.bestNpaths = []\n",
    "        self.buildTree(self.root, 0)\n",
    "\n",
    "        \n",
    "        # preforming requested tasks for sentence generation\n",
    "        if return_sentences:\n",
    "            if num_best_sentences is None:\n",
    "                print(\"Provide the number of best sentences to return\")\n",
    "                return\n",
    "            \n",
    "            self.getNBestPaths(num_best_sentences)\n",
    "            self.printBestNPaths()\n",
    "    \n",
    "    \n",
    "    def buildTree(self, node, depth):\n",
    "\n",
    "        ''' \n",
    "        Function to build the tree based on the given root and max_depth\n",
    "        '''\n",
    "\n",
    "        if depth == self.max_depth:\n",
    "            return\n",
    "        \n",
    "        k_best = self.getKBest(self.model, self.beam_size, [node.state])\n",
    "\n",
    "        for k in k_best:\n",
    "            if k is not None:\n",
    "                child = Node(parent=node, state=k)\n",
    "                node.addChild(child)\n",
    "                self.buildTree(child, depth+1)\n",
    "\n",
    "    \n",
    "    def getAllPaths(self, node, path):\n",
    "\n",
    "        '''\n",
    "        Function to get all the paths from the root to the leaf nodes\n",
    "        '''\n",
    "\n",
    "        if node.isLeaf():\n",
    "\n",
    "            pathscore = 1\n",
    "            \n",
    "            for i in range(1, len(path)):\n",
    "                pathscore *= self.model.score(path[i], [path[i-1]])\n",
    "            \n",
    "            self.pathsWithScore[tuple(path)] = pathscore   \n",
    "            return\n",
    "\n",
    "        for child in node.children:\n",
    "            self.getAllPaths(child, path + [child.state])\n",
    "    \n",
    "    \n",
    "    def getNBestPaths(self, N):\n",
    "\n",
    "        '''\n",
    "        Function to get the best N paths based on the scores\n",
    "        '''\n",
    "\n",
    "        self.getAllPaths(self.root, [self.root.state])\n",
    "        best_paths = sorted(self.pathsWithScore.items(), key=lambda item:item[1], reverse=True)\n",
    "        self.bestNpaths = best_paths[:N]\n",
    "\n",
    "    \n",
    "    def printBestNPaths(self):\n",
    "\n",
    "        '''\n",
    "        Function to print the best N paths\n",
    "        '''\n",
    "\n",
    "        for i in range(len(self.bestNpaths)):\n",
    "            print(\"Path: \", self.printSentence(self.bestNpaths[i][0]))\n",
    "            print(\"Score: \", self.bestNpaths[i][1])\n",
    "            print(\"\\n\")\n",
    "\n",
    "    \n",
    "    def printSentence(self, pathtuple):\n",
    "\n",
    "        '''\n",
    "        Function to get the sentence from the path\n",
    "        '''\n",
    "\n",
    "        sentence = pathtuple[0]\n",
    "        for word in pathtuple[1:]:\n",
    "            sentence += ' ' + word\n",
    "        \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  harry had a large pink beach ball wearing an emerald green\n",
      "Score:  4.1452323609999957e-07\n",
      "\n",
      "\n",
      "Path:  harry was a large pink beach ball wearing an emerald green\n",
      "Score:  2.6321621231072855e-07\n",
      "\n",
      "\n",
      "Path:  harry had a large pink beach ball wearing an emerald one\n",
      "Score:  2.0726161804999978e-07\n",
      "\n",
      "\n",
      "Path:  harry was a large pink beach ball wearing an emerald one\n",
      "Score:  1.3160810615536428e-07\n",
      "\n",
      "\n",
      "Path:  harry had been a large pink beach ball wearing an emerald\n",
      "Score:  9.420982638636354e-08\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bsobj = BeamSearchV2(bigram_mle, k_top_probable, 'harry', 2, 10, return_sentences=True, num_best_sentences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  dumbledore and a large pink beach ball wearing an emerald green\n",
      "Score:  1.4618533099840802e-07\n",
      "\n",
      "\n",
      "Path:  dumbledore and a large pink beach ball wearing an emerald one\n",
      "Score:  7.309266549920401e-08\n",
      "\n",
      "\n",
      "Path:  dumbledore you know who was a large pink beach ball wearing\n",
      "Score:  4.470384040603406e-08\n",
      "\n",
      "\n",
      "Path:  dumbledore and dudley had a large pink beach ball wearing a\n",
      "Score:  4.111462434330226e-08\n",
      "\n",
      "\n",
      "Path:  dumbledore you cant blame her sister marge who lived chapter the\n",
      "Score:  2.422621470240518e-08\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bsobj = BeamSearchV2(bigram_mle, k_top_probable, 'dumbledore', 2, 10, return_sentences=True, num_best_sentences=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
