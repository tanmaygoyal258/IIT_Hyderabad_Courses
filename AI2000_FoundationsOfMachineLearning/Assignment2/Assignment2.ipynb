{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI2000- Foundations of Machine Learning\n",
    "## Assignment 2\n",
    "### Tanmay Goyal\n",
    "### AI20BTECH11021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Support Vector Machines: \n",
    "#### In the derivation for SVM, if the margin boundaries are given by $\\vec{w}.\\vec{x} + b = +\\gamma$ and $\\vec{w}.\\vec{x} + b = -\\gamma$, where $\\gamma > 0$, show that the solution for the maximum margin hyperplane is unchanged.\n",
    "\n",
    "Answer: Consider  $\\vec{w}.\\vec{x} + b = +\\gamma$ and $\\vec{w}.\\vec{x} + b = -\\gamma$, where $\\gamma > 0$\n",
    "\n",
    "Let us consider a positively labelled example point as $x_+$ and a negatively labelled example point as $x_\\_$. Then we can say:\n",
    "\n",
    " $\\vec{w}.\\vec{x_+} + b = +\\gamma$ ; and $\\vec{w}.\\vec{x_\\_} + b = -\\gamma$\n",
    "\n",
    "The distance $r$ between an example point $\\vec{x}$ and seperator is given by:\n",
    "\n",
    "$r = y\\frac{\\vec{w}^T\\vec{x} + b}{||\\vec{w}||}$\n",
    "\n",
    "where $\\vec{w}$ is the weight vector and y = {+1 , -1}\n",
    "\n",
    "Thus, the margin is given by:\n",
    "\n",
    "$r = (+1)\\frac{\\vec{w}^T\\vec{x_+} + b}{||\\vec{w}||} + (-1)\\frac{\\vec{w}^T\\vec{x_\\_} + b}{||\\vec{w}||}  = (+1)\\frac{+\\gamma}{||\\vec{w}||} + (-1)\\frac{-\\gamma}{||\\vec{w||}} = \\frac{2\\gamma}{||\\vec{w}||}$\n",
    "\n",
    "To maximise the margin, we have to find $\\vec{w}$ and $b$ which will maximise $\\frac{2\\gamma}{||\\vec{w}||}$ subject to the constraints: $\\vec{w}^T\\vec{x_i} + b \\geq \\gamma$ if $y_i = +1$ and $\\vec{w}^T\\vec{x_i} + b \\leq \\gamma$ if $y_i = -1$\n",
    "\n",
    "$\\implies$ We have to find $\\vec{w}$ and $b$ which will maximise $\\frac{2}{\\frac{||\\vec{w}||}{|\\gamma|}}$  subject to the constraints: $\\frac{\\vec{w}^T}{\\gamma}\\vec{x_i} + \\frac{b}{\\gamma} \\geq 1$ if $y_i = +1$ and $\\frac{\\vec{w}^T}{\\gamma}\\vec{x_i} + \\frac{b}{\\gamma} \\leq 1$ if $y_i = -1$ (since $\\gamma>0$) \n",
    "\n",
    "Define $\\vec{w}^* = \\frac{1}{\\gamma}\\vec{w}$ and $b^* = \\frac{1}{\\gamma}b$ $\\implies ||\\vec{w}^*|| = \\frac{||\\vec{w}||}{|\\gamma|}$. Then, our problem reduces to:\n",
    "\n",
    "Find $\\vec{w}^*$ and $b*$ that maximises $\\frac{2}{||\\vec{w}^*||}$ subject to the constraints: $\\vec{w}^{*T}\\vec{x_i} + b^* \\geq 1$ if $y_i = +1$ and $\\vec{w}^{*T}\\vec{x_i} + b^* \\leq 1$ if $y_i = -1$\n",
    "\n",
    "which is exactly the solution for the maximum margin hyperplane for hard-margin SVMs.\n",
    "\n",
    "Hence, proved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Support Vector Machines: \n",
    "#### Consider the half margin of maximum-margin SVM given by $\\rho = \\frac{1}{||\\vec{w}||}$. Show that \n",
    "\n",
    "$\\frac{1}{\\rho^2} = \\sum_{i = 1}^N \\alpha_i$\n",
    "\n",
    "where $\\alpha_i$ are the Lagrange Multipliers.\n",
    "\n",
    "Answer: \n",
    "\n",
    "We have to prove : $||\\vec{w}||^2_2 =\\sum_{i = 1}^N \\alpha_i$\n",
    "\n",
    "We know that the result for SVMs are:\n",
    "$\\vec{w} = \\sum_{i=1}^R \\alpha_iy_i\\vec{x_i}$ and $\\sum_{i=1}^R\\alpha_iy_i = 0$ where both the summations are summed over only the Support Vectors.\n",
    "\n",
    "Also, since, Support Vectors lie exactly on the margins, $y_i(\\vec{w}^T\\vec{x_i} + b) = 1$.\n",
    "\n",
    "Thus,\n",
    "\n",
    "$\\vec{w} = \\sum_{i=1}^R \\alpha_iy_ix_i$\n",
    "\n",
    "$\\implies \\vec{w}^T\\vec{w} = \\sum_{i=1}^R \\alpha_iy_i(\\vec{w}^T\\vec{x_i}) = \\sum_{i=1}^R \\alpha_i(1 - y_ib)$\n",
    "\n",
    "$\\implies \\vec{w}^T\\vec{w} = \\sum_{i=1}^R \\alpha_i - \\sum_{i=1}^R \\alpha_iy_ib  = \\sum_{i=1}^R \\alpha_i - b\\sum_{i=1}^R \\alpha_iy_i$\n",
    "\n",
    "$\\implies \\vec{w}^T\\vec{w} = ||\\vec{w}||^2_2 = \\sum_{i=1}^R \\alpha_i$ since $\\sum_{i=1}^R\\alpha_iy_i = 0$.\n",
    "\n",
    "Finally, $\\sum_{i=1}^R \\alpha_i = \\sum_{i=1}^N \\alpha_i$, since for other vectors, $\\alpha_i = 0$\n",
    "\n",
    "Thus, $||\\vec{w}||^2_2 = \\sum_{i=1}^N \\alpha_i$\n",
    "\n",
    "Hence, proved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Kernels\n",
    "\n",
    "#### Let $k_1$ and $k_2$ be valid kernel functions. Comment about the validity of the following kernel functions, and justify your answer with proof or counter examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  i. $k(x,z) = k_1(x,z) + k_2(x,z) $ \n",
    "Answer: <b>Valid</b>\n",
    "\n",
    "Since $k_1$ and $k_2$ are valid kernels, from Mercer's Theorem we can say for some $f(x)$ where $\\int f^2(x) \\, dx $ is finite:\n",
    "$\\int f(x)k_1(x,z)f(z) \\,dxdz \\geq 0$ and $\\int f(x)k_2(x,z)f(z) \\,dxdz \\geq 0$\n",
    "\n",
    "Then,\n",
    "$\\int f(x)k(x,z)f(z) \\,dxdz  = \\int f(x)k_1(x,z)f(z) \\,dxdz  + \\int f(x)k_2(x,z)f(z) \\,dxdz \\geq 0$\n",
    "\n",
    "Thus, from Mercer's condition, it is a valid kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ii. $k(x,z) = k_1(x,z)k_2(x,z)$\n",
    "Answer: <b>Valid</b>. \n",
    "\n",
    "From Mercer's condition, we can write any valid kernel function as a dot product of two mappings. Let\n",
    "$k_1(x,z) = a(x).a(z)$ and $k_2(x,z) = b(x).b(z)$ and $d$ and $r$ be the dimensions of the mapping.\n",
    "\n",
    "Then, \n",
    "$k(x,z) = k_1(x,z)k_2(x,z) = \\sum_{m=1}^d \\{a_m(x)a_m(z)\\} \\times \\sum_{n=1}^r \\{b_n(x)b_n(z)\\}$\n",
    "\n",
    "$ = \\sum_{m=1}^d \\sum_{n=1}^r[a_m(x)b_n(x)][a_m(z)b_n(z)]$\n",
    "\n",
    "$ = \\sum_{m=1}^d \\sum_{n=1}^rp_{mn}(x)p_{mn}(z)$\n",
    "\n",
    "$  = p(x) . p(z)$\n",
    "\n",
    "where $p(x)$ is a $dr$ dimensional vector and $p_{mn}(x) = a_m(x)b_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. $k(x,z) =  h(k_1(x,z))$ where $h$ is a polynomial function with positive co- efficients.\n",
    "Answer: <b>Valid</b>.\n",
    "\n",
    "Consider $h(x) = a_nx^n + a_{n-1}x^{n-1} + \\ldots +a_1x + a_0$ where $a_i \\geq 0 \\forall i$\n",
    "\n",
    "Then, $h(k_1(x,z)) = a_nk_1^n(x,z) + a_{n-1}k_1^{n-1}(x,z) + \\ldots +a_1k_1(x,z) + a_0$\n",
    "\n",
    "Using part $ii$, we can say $k_1^2(x,z) = k_1(x,z)k_1(x,z)$ is a valid kernel function. Continuing in the same way, $k_1^3 , \\ldots ,k_1^n$ are also valid kernel functions.\n",
    "\n",
    "If $k_1(x,z)$ is a valid kernel function, we know $\\int f(x)k_1(x,z)f(z) \\,dxdz \\geq 0$. Multiplying $k_1$ with $\\alpha > 0$, we see $\\alpha k(x,z)$ still remains a valid kernel function as $\\int f(x)\\alpha k_1(x,z)f(z) \\,dxdz \\geq 0$\n",
    "\n",
    "Thus, each term of $h(k_1(x,z))$ is a valid kernel function in itself. \n",
    "\n",
    "Finally, using part $i$, we can say the addition of these individual valid kernel function also results in a valid kernel fucntion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv. $k(x,z) = exp(k_1(x,z))$\n",
    "Answer: <b>Valid</b>.\n",
    "\n",
    "From Taylor Series Expansion, we know:\n",
    "\n",
    "$exp(x) = \\sum_{n=0}^\\infty \\frac{x^n}{n!}$\n",
    "$\\implies exp(k_1(x,z)) = \\sum_{n=0}^\\infty \\frac{k_1^n(x,z)}{n!}$\n",
    "\n",
    "Thus, the exponential function can be represented as a infinite degree polynomial with positive coefficients, and from part $iii$, we can thus say, it would be a valid kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v. $k(x,z) = exp(\\frac{-||\\vec{x} - \\vec{z}||^2_2}{\\sigma^2})$\n",
    "Answer: <b>Valid</b>.\n",
    "\n",
    "Without loss of Generality, let us assume $\\sigma^2 = 1$\n",
    "\n",
    "$||\\vec{x} - \\vec{z}||^2_2 = (\\vec{x} - \\vec{z})^T(\\vec{x} - \\vec{z}) = \\vec{x}^T\\vec{x} -2\\vec{x}^T\\vec{z} + \\vec{z}^T\\vec{z}$\n",
    "\n",
    "Since each term can be written in terms of an inner product, and hence, a dot product, each individual term is a valid kernel, and from part $i$, the addition of these kernels results in a valid kernel. Finally, using part $iv$, the exponent of a valid kernel function results in a valid kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. SVMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching the data and preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "training_data = pd.read_table(\"http://www.amlbook.com/data/zip/features.train\" , delimiter = \"   \" , names = ['Label' , 'Features'])\n",
    "new = training_data[\"Features\"].str.split(\" \", n = 1, expand = True)\n",
    "training_data[\"Feature1\"] = new[0]\n",
    "training_data[\"Feature2\"] = new[1]\n",
    "training_data.drop(columns = [\"Features\"] , inplace = True)\n",
    "training_data = training_data.astype({\"Label\": int , \"Feature1\":float , \"Feature2\":float})\n",
    "\n",
    "test_data = pd.read_table(\"http://www.amlbook.com/data/zip/features.test\" , delimiter = \"   \" , names = ['Label' , 'Features'])\n",
    "new = test_data[\"Features\"].str.split(\" \", n = 1, expand = True)\n",
    "test_data[\"Feature1\"] = new[0]\n",
    "test_data[\"Feature2\"] = new[1]\n",
    "test_data.drop(columns = [\"Features\"] , inplace = True)\n",
    "test_data = test_data.astype({\"Label\": int , \"Feature1\":float , \"Feature2\":float} )\n",
    "\n",
    "# extracting the rows with labels as 1 or 5\n",
    "training_data = training_data.loc[(training_data[\"Label\"]==1) | (training_data[\"Label\"]==5)]\n",
    "test_data = test_data.loc[(test_data[\"Label\"]==1) | (test_data[\"Label\"]==5)]\n",
    "\n",
    "# changing the 1s to +1 and 5s to -1\n",
    "training_data['Label'] = training_data['Label'].replace([1,5] , [+1 , -1])\n",
    "test_data['Label'] = test_data['Label'].replace([1,5] , [+1 , -1])\n",
    "\n",
    "# extracting final training and testing data\n",
    "X_train = training_data[[\"Feature1\" , \"Feature2\"]].to_numpy()\n",
    "X_test = test_data[[\"Feature1\" , \"Feature2\"]].to_numpy()\n",
    "y_train = training_data['Label'].to_numpy()\n",
    "y_test = test_data['Label'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Accuracy over entire test data is: 97.87735849056604\n",
      "Number of support vectors are: 28\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# part (a)\n",
    "model = svm.SVC(kernel = 'linear')\n",
    "model.fit(X_train , y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "accuracy = ((np.sum(y_predicted==y_test))/len(y_test))\n",
    "\n",
    "print(\"*\"*75)\n",
    "print(\"Accuracy over entire test data is: {}\".format(accuracy*100))\n",
    "print(\"Number of support vectors are: {}\".format(len(model.support_vectors_)))\n",
    "print(\"*\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "For 50 training samples , accuracy over entire test data is: 98.11320754716981\n",
      "For 50 training samples , number of support vectors are: 2\n",
      "For 100 training samples , accuracy over entire test data is: 98.11320754716981\n",
      "For 100 training samples , number of support vectors are: 4\n",
      "For 200 training samples , accuracy over entire test data is: 98.11320754716981\n",
      "For 200 training samples , number of support vectors are: 8\n",
      "For 800 training samples , accuracy over entire test data is: 98.11320754716981\n",
      "For 800 training samples , number of support vectors are: 14\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# part (b)\n",
    "Number_training_samples = [50,100,200,800]\n",
    "print(\"*\"*75)\n",
    "\n",
    "for n in Number_training_samples:\n",
    "    model = svm.SVC(kernel = 'linear')\n",
    "    model.fit(X_train[:n], y_train[:n])\n",
    "    y_predicted = model.predict(X_test)\n",
    "    accuracy = ((np.sum(y_predicted==y_test))/len(y_test))\n",
    "    print(\"For {} training samples , accuracy over entire test data is: {}\".format(n,accuracy*100))\n",
    "    print(\"For {} training samples , number of support vectors are: {}\".format(n,len(model.support_vectors_)))\n",
    "\n",
    "print(\"*\"*75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "For C = 0.0001 and Q = 2 , training error is: 0.008968609865470878\n",
      "For C = 0.0001 and Q = 2 , testing error over entire test data is: 0.01650943396226412\n",
      "For C = 0.0001 and Q = 2 , number of support vectors are: 236\n",
      "For C = 0.0001 and Q = 5 , training error is: 0.004484304932735439\n",
      "For C = 0.0001 and Q = 5 , testing error over entire test data is: 0.018867924528301883\n",
      "For C = 0.0001 and Q = 5 , number of support vectors are: 26\n",
      "For C = 0.001 and Q = 2 , training error is: 0.004484304932735439\n",
      "For C = 0.001 and Q = 2 , testing error over entire test data is: 0.01650943396226412\n",
      "For C = 0.001 and Q = 2 , number of support vectors are: 76\n",
      "For C = 0.001 and Q = 5 , training error is: 0.004484304932735439\n",
      "For C = 0.001 and Q = 5 , testing error over entire test data is: 0.021226415094339646\n",
      "For C = 0.001 and Q = 5 , number of support vectors are: 25\n",
      "For C = 0.01 and Q = 2 , training error is: 0.004484304932735439\n",
      "For C = 0.01 and Q = 2 , testing error over entire test data is: 0.018867924528301883\n",
      "For C = 0.01 and Q = 2 , number of support vectors are: 34\n",
      "For C = 0.01 and Q = 5 , training error is: 0.0038436899423446302\n",
      "For C = 0.01 and Q = 5 , testing error over entire test data is: 0.021226415094339646\n",
      "For C = 0.01 and Q = 5 , number of support vectors are: 23\n",
      "For C = 1 and Q = 2 , training error is: 0.0032030749519538215\n",
      "For C = 1 and Q = 2 , testing error over entire test data is: 0.018867924528301883\n",
      "For C = 1 and Q = 2 , number of support vectors are: 24\n",
      "For C = 1 and Q = 5 , training error is: 0.0032030749519538215\n",
      "For C = 1 and Q = 5 , testing error over entire test data is: 0.021226415094339646\n",
      "For C = 1 and Q = 5 , number of support vectors are: 21\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# part(c)\n",
    "\n",
    "# given in Question: polynomial_kernel = (1 + x.x')^Q\n",
    "# default polynomial kernel in sklearn = (coef0 + gamma * (x.x'))^degree\n",
    "# therefore: coef0 = 1 , gamma = 1 , degree = Q\n",
    "\n",
    "degrees = [2 , 5]\n",
    "c_values = [0.0001 , 0.001 , 0.01 , 1]\n",
    "print(\"*\"*75)\n",
    "\n",
    "for c_value in c_values:\n",
    "    for Q in degrees:\n",
    "        model = svm.SVC(kernel = 'poly' , gamma = 1 , coef0 = 1 , degree = Q, C = c_value  )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predicted_train = model.predict(X_train)\n",
    "        y_predicted_test = model.predict(X_test)\n",
    "        training_error = 1 - ((np.sum(y_predicted_train==y_train))/len(y_train))\n",
    "        testing_error = 1 - ((np.sum(y_predicted_test==y_test))/len(y_test))\n",
    "        print(\"For C = {} and Q = {} , training error is: {}\".format(c_value , Q,training_error))\n",
    "        print(\"For C = {} and Q = {} , testing error over entire test data is: {}\".format(c_value , Q,testing_error))\n",
    "        print(\"For C = {} and Q = {} , number of support vectors are: {}\".format(c_value , Q,len(model.support_vectors_)))\n",
    "\n",
    "print(\"*\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Which of these are true:\n",
    "\n",
    "#### i. When $C = 0.0001$, training error is high at $Q = 5$\n",
    "Answer: False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ii. When $C = 0.001$, the number of support vectors are lower at $Q = 5$\n",
    "Answer: True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### iii. When $C = 0.01$, the  training error is higher at $Q = 5$\n",
    "Answer: False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### iv. When $C = 1$, testing error is lower at $Q = 5$\n",
    "Answer: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "For C = 0.01 , training error is: 0.0038436899423446302\n",
      "For C = 0.01 , testing error over entire test data is: 0.02358490566037741\n",
      "For C = 1 , training error is: 0.004484304932735439\n",
      "For C = 1 , testing error over entire test data is: 0.021226415094339646\n",
      "For C = 100 , training error is: 0.0032030749519538215\n",
      "For C = 100 , testing error over entire test data is: 0.018867924528301883\n",
      "For C = 10000 , training error is: 0.002562459961563124\n",
      "For C = 10000 , testing error over entire test data is: 0.02358490566037741\n",
      "For C = 1000000 , training error is: 0.0006406149903908087\n",
      "For C = 1000000 , testing error over entire test data is: 0.02358490566037741\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# part (d)\n",
    "\n",
    "# RBF Kernel is given by exp(-||x - x'||^2)\n",
    "# Documentation specifies RBF Kernel as exp(-gamma * ||x - x'||^2) , gamma > 0\n",
    "# Thus, gamma = 1\n",
    "\n",
    "C_values = [0.01 , 1 , 100 , 10**4 , 10**6]\n",
    "print(\"*\"*75)\n",
    "\n",
    "for c_value in C_values:\n",
    "        model = svm.SVC(kernel = 'rbf' , gamma = 1 , C = c_value)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_predicted_train = model.predict(X_train)\n",
    "        y_predicted_test = model.predict(X_test)\n",
    "        training_error = 1 - ((np.sum(y_predicted_train==y_train))/len(y_train))\n",
    "        testing_error = 1 - ((np.sum(y_predicted_test==y_test))/len(y_test))\n",
    "        print(\"For C = {} , training error is: {}\".format(c_value , training_error))\n",
    "        print(\"For C = {} , testing error over entire test data is: {}\".format(c_value ,testing_error))\n",
    "        \n",
    "print(\"*\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) For what value of C is the training error the lowest? And what about the testing error?\n",
    "Answer: The lowest training error is for $C = 10^6$ while the lowest testing error is for $C = 100$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. SVMs continued: GISETTE dataset\n",
    "datasource: (https://archive.ics.uci.edu/ml/datasets/Gisette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data has shape: (6000, 5000)\n",
      "Number of training labels are : (6000,)\n",
      "The testing data has shape: (1000, 5000)\n",
      "Number of training labels are : (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "# genfromtxt uses whitespace as a default delimiter\n",
    "\n",
    "X_train= np.loadtxt(\"gisette_train.data\") \n",
    "print(\"The training data has shape: {}\".format (X_train.shape))\n",
    "\n",
    "y_train = np.loadtxt(\"gisette_train.labels\") \n",
    "print(\"Number of training labels are : {}\".format(y_train.shape))\n",
    "\n",
    "X_test = np.loadtxt(\"gisette_valid.data\") \n",
    "print(\"The testing data has shape: {}\".format(X_test.shape))\n",
    "\n",
    "y_test = np.loadtxt(\"gisette_valid.labels\") \n",
    "print(\"Number of training labels are : {}\".format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "The training error is: 0.0\n",
      "The testing error over entire test data is: 0.02400000000000002\n",
      "The number of support vectors are: 1084\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# part (a)\n",
    "print(\"*\"*75)\n",
    "\n",
    "model = svm.SVC(kernel = 'linear')\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted_train = model.predict(X_train)\n",
    "y_predicted_test = model.predict(X_test)\n",
    "training_error = 1 - ((np.sum(y_predicted_train==y_train))/len(y_train))\n",
    "testing_error = 1 - ((np.sum(y_predicted_test==y_test))/len(y_test))\n",
    "print(\"The training error is: {}\".format(training_error))\n",
    "print(\"The testing error over entire test data is: {}\".format(testing_error))\n",
    "print(\"The number of support vectors are: {}\".format(len(model.support_vectors_)))\n",
    "\n",
    "print(\"*\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "For Gaussian kernel, the training error is: 0.0\n",
      "For Gaussian kernel, the testing error over entire test data is: 0.5\n",
      "For Gaussian kernel, the number of support vectors are: 6000\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# part (b)\n",
    "print(\"*\"*75)\n",
    "\n",
    "model = svm.SVC(kernel = 'rbf' , gamma = 0.001)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_predicted_train = model.predict(X_train)\n",
    "y_predicted_test = model.predict(X_test)\n",
    "\n",
    "training_error = 1 - ((np.sum(y_predicted_train==y_train))/len(y_train))\n",
    "testing_error = 1 - ((np.sum(y_predicted_test==y_test))/len(y_test))\n",
    "print(\"For Gaussian kernel, the training error is: {}\".format(training_error))\n",
    "print(\"For Gaussian kernel, the testing error over entire test data is: {}\".format(testing_error))\n",
    "print(\"For Gaussian kernel, the number of support vectors are: {}\".format(len(model.support_vectors_)))\n",
    "\n",
    "print(\"*\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "For polynomial kernel, the training error is: 0.0004999999999999449\n",
      "For polynomial kernel, the testing error over entire test data is: 0.020000000000000018\n",
      "For polynomial kernel, the number of support vectors are: 1332\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"*\"*75)\n",
    "\n",
    "model = svm.SVC(kernel = 'poly' , degree = 2 , coef0 = 1)\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted_train = model.predict(X_train)\n",
    "y_predicted_test = model.predict(X_test)\n",
    "training_error = 1 - ((np.sum(y_predicted_train==y_train))/len(y_train))\n",
    "testing_error = 1 - ((np.sum(y_predicted_test==y_test))/len(y_test))\n",
    "print(\"For polynomial kernel, the training error is: {}\".format(training_error))\n",
    "print(\"For polynomial kernel, the testing error over entire test data is: {}\".format(testing_error))\n",
    "print(\"For polynomial kernel, the number of support vectors are: {}\".format(len(model.support_vectors_)))\n",
    "\n",
    "print(\"*\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest training error is for the Gaussian kernel."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bec9f652a4082306f869e119250ed899f786ad91c86aea518642b2537134d841"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
