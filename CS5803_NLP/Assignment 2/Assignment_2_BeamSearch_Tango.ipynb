{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS5803 NLP\n",
    "### Assignment 2\n",
    "#### Tanmay Garg, Tanmay Goyal, Tanay Yadav\n",
    "#### Roll no: CS20BTECH11063, AI20BTECH11021, AI20BTECH11026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Link to dataset: https://www.kaggle.com/datasets/moxxis/harry-potter-lstm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tanmaygoyal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tanmaygoyal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing necessary packages\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from math import log, exp  \n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1. Preprocess and tokenize the dataset using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the original data\n",
    "with open('Harry_Potter_all_char_separated.txt', 'r', encoding='utf-8') as file:\n",
    "    harry_potter_data = file.read()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    '''\n",
    "    Function to preprocess the text by removing punctuations and converting to lower case\n",
    "    '''\n",
    "    # [^\\w\\s] -> ^ means except , \\w refers to any alphanumeric character and \\s refers to whitespace    \n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    '''\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    # stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# preprocessing the text\n",
    "harry_potter_text = preprocess_text(harry_potter_data)\n",
    "# using the first 10000 words\n",
    "harry_potter_tokens = tokenize(harry_potter_text)[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2. Fit two bigram language models on the text: MLE and kneserNey Discounting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit two bigram language models on the text: MLE and Kneser-Ney discounting using the nltk library\n",
    "\n",
    "def MLE_bigram(n , n_gram , vocab):\n",
    "    '''\n",
    "    Function to fit a bigram language model using MLE\n",
    "    '''\n",
    "    model = nltk.lm.MLE(n)\n",
    "    model.fit([n_gram], vocabulary_text = vocab)\n",
    "    return model\n",
    "\n",
    "def KN_bigram(n , n_gram , vocab):\n",
    "    '''\n",
    "    Function to fit a bigram language model using Kneser-Ney discounting\n",
    "    '''\n",
    "    model = nltk.lm.KneserNeyInterpolated(n)\n",
    "    model.fit([n_gram], vocabulary_text = vocab)\n",
    "    return model\n",
    "\n",
    "harry_potter_bigrams = nltk.ngrams(harry_potter_tokens, 2)\n",
    "\n",
    "# converting the bigrams to a list\n",
    "bigrams = []\n",
    "for bigram in harry_potter_bigrams:\n",
    "    bigrams.append(bigram)\n",
    "\n",
    "bigram_mle = MLE_bigram(2 , bigrams , harry_potter_tokens)\n",
    "bigram_kn = KN_bigram(2 , bigrams , harry_potter_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3. Use the beginning words 1. \"Harry Potter\" and 2. \"Dumbledore\" to generate text using both the language models. Keep maximum text length as 20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== MLE Model ====\n",
      "harry potter are are less like us all right of them angrily it he does tend to be nice if hed just\n",
      "dumbledore and bacon as dudley was a man had swapped at his favorite program had expected mrs dursleys bought dudley he\n",
      "==== KneserNey Model ====\n",
      "harry potter are are less like us all right place you cant ive finished dialing his hair and piers and had four\n",
      "dumbledore and bacon as dudley was a mad old things gray tuesday our heads down old clothes of arms and james\n"
     ]
    }
   ],
   "source": [
    "def generate_prediction(model , num_words = None , text_seed = None , random_seed = None):\n",
    "    '''\n",
    "    Function to generate the predictions given text_seed and num_words.\n",
    "    It joins them together in a sentence and returns the sentence\n",
    "    '''\n",
    "\n",
    "    # preprocessing the text_seed\n",
    "    text_seed = preprocess_text(text_seed)\n",
    "    text_seed = tokenize(text_seed)\n",
    "    \n",
    "    # generating the prediction\n",
    "    pred = model.generate(num_words = num_words , text_seed = text_seed , random_seed = random_seed)\n",
    "    \n",
    "    sentence = text_seed + [pred[i] for i in range(num_words)]\n",
    "    predicted_sentence = sentence[0]\n",
    "    for word in sentence[1:]:\n",
    "        predicted_sentence += ' ' + word\n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "print(\"==== MLE Model ====\")\n",
    "print(generate_prediction(bigram_mle , num_words = 20 , text_seed = \"Harry Potter\" , random_seed = 123))\n",
    "print(generate_prediction(bigram_mle , num_words = 20 , text_seed = \"Dumbledore\" , random_seed = 123)) \n",
    "\n",
    "print(\"==== KneserNey Model ====\")\n",
    "print(generate_prediction(bigram_kn , num_words = 20 , text_seed = \"Harry Potter\" , random_seed = 123))\n",
    "print(generate_prediction(bigram_kn , num_words = 20 , text_seed = \"Dumbledore\" , random_seed = 123)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search is a tree-based search strategy similar to BFS. In BF, we expand every child node, however, in Beam Search, we only expand the top k most probable children. The generated text is the text with the highest probabiltity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4. To implement beam search, implement a function to find the top k most probable words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_top_probable(model , k , text_seed):\n",
    "    '''\n",
    "    returns the top-k most probable words based on the model given\n",
    "    ''' \n",
    "    \n",
    "    # preprocessing the text_seed\n",
    "    text_seed = [w.lower() for w in text_seed]\n",
    "    # we store the non-zero probabilities\n",
    "    non_zero_prob = {}\n",
    "\n",
    "    for w in model.vocab:\n",
    "        if model.score(w , text_seed) > 0:\n",
    "            non_zero_prob[w] = model.score(w , text_seed)\n",
    "\n",
    "    # sorting the non_zero_prob based on values\n",
    "    sorted_probabilities = dict(sorted(non_zero_prob.items() , key = lambda item:item[1] , reverse = True))\n",
    "    if len(non_zero_prob) > k:\n",
    "        return list(sorted_probabilities.keys())[:k]\n",
    "    \n",
    "    else:\n",
    "        top = list(sorted_probabilities.keys())\n",
    "\n",
    "        return top + [None]*(k - len(top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5. Implement the Beam search using the previously trained MLE model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode():\n",
    "\n",
    "    def __init__(self , current_sentence , probability = 1 , depth = 0):\n",
    "        self.current_sentence = current_sentence\n",
    "        self.children = []\n",
    "        self.probability = probability\n",
    "        self.depth = depth\n",
    "\n",
    "    def add_child(self,child):\n",
    "        self.children.append(child)\n",
    "        return\n",
    "\n",
    "\n",
    "class BeamSearch:\n",
    "\n",
    "    '''\n",
    "    class for implementing BeamSearch to generate a sentence using the Ngram Language Modelling\n",
    "\n",
    "    input parameters:\n",
    "        model = Ngram Language Model\n",
    "        topKfunction = Function to get the top-k probable words\n",
    "        k = Top k-probable words to consider\n",
    "        starting_context = The starting context for the sentence\n",
    "        ngram_N = The value of n for n-grams, example n=1 for unigrams\n",
    "        max_depth = The maximm depth of the tree\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self , model , topKfunction , k , context , ngram_N , max_depth):\n",
    "        self.model = model\n",
    "        self.k = k\n",
    "        self.context = [context]\n",
    "        self.topKfunction = topKfunction\n",
    "        self.n = ngram_N\n",
    "        self.max_depth = max_depth\n",
    "        self.generated_sentences = {} \n",
    "        self.queue = []\n",
    "        self.root = BeamSearchNode(self.context)\n",
    "        self.queue.append(self.root)\n",
    "        self.tree = self.generate_tree(self.root)\n",
    "\n",
    "    def generate_tree(self , node ):\n",
    "        '''\n",
    "        Function to generate the tree\n",
    "        '''\n",
    "        self.queue = self.queue[1:]\n",
    "\n",
    "        if node.depth == self.max_depth:\n",
    "            self.generated_sentences[tuple(node.current_sentence)] = node.probability\n",
    "            if len(self.queue) != 0:\n",
    "                self.generate_tree(self.queue[0])\n",
    "                return\n",
    "            else:\n",
    "                return\n",
    "\n",
    "\n",
    "        context_to_generate_words = node.current_sentence[-(self.n-1):] if len(node.current_sentence) >= self.n-1 else node.current_sentence\n",
    "        context_to_generate_words = tuple(context_to_generate_words)\n",
    "\n",
    "        top_prob_words = self.topKfunction(self.model , self.k , context_to_generate_words)\n",
    "\n",
    "        for word in top_prob_words:\n",
    "            if word is not None:\n",
    "                new_node = BeamSearchNode(node.current_sentence + [word] , node.probability * self.model.score(word , context_to_generate_words) , depth = node.depth+1)\n",
    "                node.add_child(new_node)\n",
    "                self.queue.append(new_node)\n",
    "\n",
    "        if len(self.queue) != 0:\n",
    "            self.generate_tree(self.queue[0])\n",
    "\n",
    "    def print_sentences(self , num_sentences):\n",
    "        \n",
    "        sorted_sentences = sorted(self.generated_sentences.items() , key = lambda item: item[1] , reverse = True)\n",
    "        \n",
    "        num_sentences = min(num_sentences , len(sorted_sentences))\n",
    "\n",
    "        for s in range(num_sentences):\n",
    "            current_sentence_tuple = sorted_sentences[s][0]\n",
    "\n",
    "            current_sentence = current_sentence_tuple[0]\n",
    "            for word in current_sentence_tuple[1:]:\n",
    "                current_sentence += ' ' + word\n",
    "\n",
    "            print(\"{}. {}\".format(s+1 , current_sentence))\n",
    "            print(\"Probability = {}\".format(sorted_sentences[s][1]))\n",
    "\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q6. Implement Beam search for k=2 and depth = 10. Find the 5 generated texts with the highest probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bigram_mle\n",
    "topKfunction = k_top_probable\n",
    "k = 2\n",
    "n = 2\n",
    "max_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. harry had a large pink beach ball wearing an emerald green\n",
      "Probability = 4.1452323609999957e-07\n",
      "2. harry was a large pink beach ball wearing an emerald green\n",
      "Probability = 2.6321621231072855e-07\n",
      "3. harry had a large pink beach ball wearing an emerald one\n",
      "Probability = 2.0726161804999978e-07\n",
      "4. harry was a large pink beach ball wearing an emerald one\n",
      "Probability = 1.3160810615536428e-07\n",
      "5. harry had been a large pink beach ball wearing an emerald\n",
      "Probability = 9.420982638636354e-08\n"
     ]
    }
   ],
   "source": [
    "b = BeamSearch(model , topKfunction , k , 'harry' , n , max_depth)\n",
    "b.print_sentences(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. dumbledore and a large pink beach ball wearing an emerald green\n",
      "Probability = 1.4618533099840802e-07\n",
      "2. dumbledore and a large pink beach ball wearing an emerald one\n",
      "Probability = 7.309266549920401e-08\n",
      "3. dumbledore you know who was a large pink beach ball wearing\n",
      "Probability = 4.470384040603406e-08\n",
      "4. dumbledore and dudley had a large pink beach ball wearing a\n",
      "Probability = 4.111462434330226e-08\n",
      "5. dumbledore you cant blame her sister marge who lived chapter the\n",
      "Probability = 2.422621470240518e-08\n"
     ]
    }
   ],
   "source": [
    "b = BeamSearch(model , topKfunction , k , 'dumbledore' , n , max_depth)\n",
    "b.print_sentences(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
